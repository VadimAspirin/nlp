{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "task1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "BWSRGhKL3cpq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!git clone https://github.com/Samsung-IT-Academy/stepik-dl-nlp.git && pip install -r stepik-dl-nlp/requirements.txt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I16MHqRD3jTd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sys; sys.path.append('./stepik-dl-nlp')\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import sklearn\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "import numpy as np\n",
        "import scipy.sparse\n",
        "import pandas as pd\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "import collections\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "import dlnlputils\n",
        "from dlnlputils.data import tokenize_text_simple_regex, tokenize_corpus, build_vocabulary, vectorize_texts, SparseFeaturesDataset\n",
        "from dlnlputils.pipeline import train_eval_loop, predict_with_model, init_random_seed\n",
        "\n",
        "init_random_seed()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cr-0l967LqhV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "results = []"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6tB97gSP4j8A",
        "colab_type": "text"
      },
      "source": [
        "#VECTORIZATION_MODE: tfidf"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X7gOsWaar_xt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "VECTORIZATION_MODE = 'tfidf'\n",
        "LR = 1e-1\n",
        "L2 = 0\n",
        "BATCH = 32\n",
        "EPOCH = 200"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jgl_7JDW3qR2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_source = fetch_20newsgroups(subset='train')\n",
        "test_source = fetch_20newsgroups(subset='test')\n",
        "\n",
        "train_tokenized = tokenize_corpus(train_source['data'])\n",
        "test_tokenized = tokenize_corpus(test_source['data'])\n",
        "\n",
        "MAX_DF = 0.8\n",
        "MIN_COUNT = 5\n",
        "vocabulary, word_doc_freq = build_vocabulary(train_tokenized, max_doc_freq=MAX_DF, min_count=MIN_COUNT)\n",
        "\n",
        "train_vectors = vectorize_texts(train_tokenized, vocabulary, word_doc_freq, mode=VECTORIZATION_MODE)\n",
        "test_vectors = vectorize_texts(test_tokenized, vocabulary, word_doc_freq, mode=VECTORIZATION_MODE)\n",
        "\n",
        "train_dataset = SparseFeaturesDataset(train_vectors, train_source['target'])\n",
        "test_dataset = SparseFeaturesDataset(test_vectors, test_source['target'])\n",
        "\n",
        "UNIQUE_WORDS_N = len(vocabulary)\n",
        "UNIQUE_LABELS_N = len(set(train_source['target']))\n",
        "model = nn.Linear(UNIQUE_WORDS_N, UNIQUE_LABELS_N)\n",
        "\n",
        "scheduler = lambda optim: torch.optim.lr_scheduler.ReduceLROnPlateau(optim, patience=5, factor=0.5, verbose=True)\n",
        "\n",
        "best_val_loss, best_model = train_eval_loop(model=model,\n",
        "                                            train_dataset=train_dataset,\n",
        "                                            val_dataset=test_dataset,\n",
        "                                            criterion=F.cross_entropy,\n",
        "                                            lr=LR,\n",
        "                                            epoch_n=EPOCH,\n",
        "                                            batch_size=BATCH,\n",
        "                                            l2_reg_alpha=L2,\n",
        "                                            lr_scheduler_ctor=scheduler)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4a_QSpSKKBT8",
        "colab_type": "code",
        "outputId": "4e73d38e-8917-4d33-a40f-3acadae8ea25",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "train_pred = predict_with_model(best_model, train_dataset)\n",
        "train_loss = F.cross_entropy(torch.from_numpy(train_pred),torch.from_numpy(train_source['target']).long())\n",
        "test_pred = predict_with_model(best_model, test_dataset)\n",
        "test_loss = F.cross_entropy(torch.from_numpy(test_pred), torch.from_numpy(test_source['target']).long())"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 354/353.5625 [00:01<00:00, 184.61it/s]\n",
            "236it [00:01, 180.98it/s]                             \n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-TTsqKJDDRet",
        "colab_type": "code",
        "outputId": "9f3ceadb-42cd-4bd7-8050-0f8267431868",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "print('Среднее значение функции потерь на обучении', float(train_loss))\n",
        "print('Доля верных ответов', accuracy_score(train_source['target'], train_pred.argmax(-1)))\n",
        "print()\n",
        "print('Среднее значение функции потерь на валидации', float(test_loss))\n",
        "print('Доля верных ответов', accuracy_score(test_source['target'], test_pred.argmax(-1)))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Среднее значение функции потерь на обучении 0.004174976609647274\n",
            "Доля верных ответов 0.9992929114371575\n",
            "\n",
            "Среднее значение функции потерь на валидации 0.9288815259933472\n",
            "Доля верных ответов 0.7712426978226234\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hTU5gG7pL-o5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "results.append({'mode': 'tfidf', 'train_accuracy': accuracy_score(train_source['target'], train_pred.argmax(-1)), 'test_accuracy': accuracy_score(test_source['target'], test_pred.argmax(-1)), 'train_loss': float(train_loss), 'test_loss': float(test_loss)})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t8bWXN2_KO5K",
        "colab_type": "text"
      },
      "source": [
        "#VECTORIZATION_MODE: tf"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HSy2jLudr9_m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "VECTORIZATION_MODE = 'tf'\n",
        "LR = 1e-1\n",
        "L2 = 0\n",
        "BATCH = 32\n",
        "EPOCH = 200"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PIGxp1FQDRhl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_source = fetch_20newsgroups(subset='train')\n",
        "test_source = fetch_20newsgroups(subset='test')\n",
        "\n",
        "train_tokenized = tokenize_corpus(train_source['data'])\n",
        "test_tokenized = tokenize_corpus(test_source['data'])\n",
        "\n",
        "MAX_DF = 0.8\n",
        "MIN_COUNT = 5\n",
        "vocabulary, word_doc_freq = build_vocabulary(train_tokenized, max_doc_freq=MAX_DF, min_count=MIN_COUNT)\n",
        "\n",
        "train_vectors = vectorize_texts(train_tokenized, vocabulary, word_doc_freq, mode=VECTORIZATION_MODE)\n",
        "test_vectors = vectorize_texts(test_tokenized, vocabulary, word_doc_freq, mode=VECTORIZATION_MODE)\n",
        "\n",
        "train_dataset = SparseFeaturesDataset(train_vectors, train_source['target'])\n",
        "test_dataset = SparseFeaturesDataset(test_vectors, test_source['target'])\n",
        "\n",
        "UNIQUE_WORDS_N = len(vocabulary)\n",
        "UNIQUE_LABELS_N = len(set(train_source['target']))\n",
        "model = nn.Linear(UNIQUE_WORDS_N, UNIQUE_LABELS_N)\n",
        "\n",
        "scheduler = lambda optim: torch.optim.lr_scheduler.ReduceLROnPlateau(optim, patience=5, factor=0.5, verbose=True)\n",
        "\n",
        "best_val_loss, best_model = train_eval_loop(model=model,\n",
        "                                            train_dataset=train_dataset,\n",
        "                                            val_dataset=test_dataset,\n",
        "                                            criterion=F.cross_entropy,\n",
        "                                            lr=LR,\n",
        "                                            epoch_n=EPOCH,\n",
        "                                            batch_size=BATCH,\n",
        "                                            l2_reg_alpha=L2,\n",
        "                                            lr_scheduler_ctor=scheduler)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X0jUvVl7KUtT",
        "colab_type": "code",
        "outputId": "2f5e952b-f95b-48c3-b6b8-0c49910e81e2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "train_pred = predict_with_model(best_model, train_dataset)\n",
        "train_loss = F.cross_entropy(torch.from_numpy(train_pred),torch.from_numpy(train_source['target']).long())\n",
        "test_pred = predict_with_model(best_model, test_dataset)\n",
        "test_loss = F.cross_entropy(torch.from_numpy(test_pred), torch.from_numpy(test_source['target']).long())"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 354/353.5625 [00:01<00:00, 180.62it/s]\n",
            "236it [00:01, 182.84it/s]                             \n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QPwR6qXADRke",
        "colab_type": "code",
        "outputId": "268c842d-557e-4714-c643-bc0387433b44",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "print('Среднее значение функции потерь на обучении', float(train_loss))\n",
        "print('Доля верных ответов', accuracy_score(train_source['target'], train_pred.argmax(-1)))\n",
        "print()\n",
        "print('Среднее значение функции потерь на валидации', float(test_loss))\n",
        "print('Доля верных ответов', accuracy_score(test_source['target'], test_pred.argmax(-1)))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Среднее значение функции потерь на обучении 0.005827018991112709\n",
            "Доля верных ответов 0.9992045253668022\n",
            "\n",
            "Среднее значение функции потерь на валидации 0.6565795540809631\n",
            "Доля верных ответов 0.8145246946362188\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xx50ILe-Mdld",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "results.append({'mode': 'tf', 'train_accuracy': accuracy_score(train_source['target'], train_pred.argmax(-1)), 'test_accuracy': accuracy_score(test_source['target'], test_pred.argmax(-1)), 'train_loss': float(train_loss), 'test_loss': float(test_loss)})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pfj2qq4WStz2",
        "colab_type": "text"
      },
      "source": [
        "#VECTORIZATION_MODE: idf"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u85FFan_r73t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "VECTORIZATION_MODE = 'idf'\n",
        "LR = 1e-1\n",
        "L2 = 0\n",
        "BATCH = 32\n",
        "EPOCH = 200"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LFVkj7zCDRm8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_source = fetch_20newsgroups(subset='train')\n",
        "test_source = fetch_20newsgroups(subset='test')\n",
        "\n",
        "train_tokenized = tokenize_corpus(train_source['data'])\n",
        "test_tokenized = tokenize_corpus(test_source['data'])\n",
        "\n",
        "MAX_DF = 0.8\n",
        "MIN_COUNT = 5\n",
        "vocabulary, word_doc_freq = build_vocabulary(train_tokenized, max_doc_freq=MAX_DF, min_count=MIN_COUNT)\n",
        "\n",
        "train_vectors = vectorize_texts(train_tokenized, vocabulary, word_doc_freq, mode=VECTORIZATION_MODE)\n",
        "test_vectors = vectorize_texts(test_tokenized, vocabulary, word_doc_freq, mode=VECTORIZATION_MODE)\n",
        "\n",
        "train_dataset = SparseFeaturesDataset(train_vectors, train_source['target'])\n",
        "test_dataset = SparseFeaturesDataset(test_vectors, test_source['target'])\n",
        "\n",
        "UNIQUE_WORDS_N = len(vocabulary)\n",
        "UNIQUE_LABELS_N = len(set(train_source['target']))\n",
        "model = nn.Linear(UNIQUE_WORDS_N, UNIQUE_LABELS_N)\n",
        "\n",
        "scheduler = lambda optim: torch.optim.lr_scheduler.ReduceLROnPlateau(optim, patience=5, factor=0.5, verbose=True)\n",
        "\n",
        "best_val_loss, best_model = train_eval_loop(model=model,\n",
        "                                            train_dataset=train_dataset,\n",
        "                                            val_dataset=test_dataset,\n",
        "                                            criterion=F.cross_entropy,\n",
        "                                            lr=LR,\n",
        "                                            epoch_n=EPOCH,\n",
        "                                            batch_size=BATCH,\n",
        "                                            l2_reg_alpha=L2,\n",
        "                                            lr_scheduler_ctor=scheduler)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TNnrAHdb3qUU",
        "colab_type": "code",
        "outputId": "2c0af359-aaad-43ab-d15b-ddcc32553c8c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "train_pred = predict_with_model(best_model, train_dataset)\n",
        "train_loss = F.cross_entropy(torch.from_numpy(train_pred),torch.from_numpy(train_source['target']).long())\n",
        "test_pred = predict_with_model(best_model, test_dataset)\n",
        "test_loss = F.cross_entropy(torch.from_numpy(test_pred), torch.from_numpy(test_source['target']).long())"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 354/353.5625 [00:01<00:00, 189.35it/s]\n",
            "236it [00:01, 188.52it/s]                             \n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VSuqTI4L3qWw",
        "colab_type": "code",
        "outputId": "ff950f2e-722d-48f3-be6a-5e97eb2aed37",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "print('Среднее значение функции потерь на обучении', float(train_loss))\n",
        "print('Доля верных ответов', accuracy_score(train_source['target'], train_pred.argmax(-1)))\n",
        "print()\n",
        "print('Среднее значение функции потерь на валидации', float(test_loss))\n",
        "print('Доля верных ответов', accuracy_score(test_source['target'], test_pred.argmax(-1)))"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Среднее значение функции потерь на обучении 0.021615294739603996\n",
            "Доля верных ответов 0.9984090507336044\n",
            "\n",
            "Среднее значение функции потерь на валидации 1.0027966499328613\n",
            "Доля верных ответов 0.7525225703664365\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0FB3eiL0MhiM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "results.append({'mode': 'idf', 'train_accuracy': accuracy_score(train_source['target'], train_pred.argmax(-1)), 'test_accuracy': accuracy_score(test_source['target'], test_pred.argmax(-1)), 'train_loss': float(train_loss), 'test_loss': float(test_loss)})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2uU5IAO6rb_d",
        "colab_type": "text"
      },
      "source": [
        "#VECTORIZATION_MODE: binary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lsgIPozqr1WQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "VECTORIZATION_MODE = 'bin'\n",
        "LR = 1e-1\n",
        "L2 = 0\n",
        "BATCH = 32\n",
        "EPOCH = 200"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7DU-Vo9SpDlq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_source = fetch_20newsgroups(subset='train')\n",
        "test_source = fetch_20newsgroups(subset='test')\n",
        "\n",
        "train_tokenized = tokenize_corpus(train_source['data'])\n",
        "test_tokenized = tokenize_corpus(test_source['data'])\n",
        "\n",
        "MAX_DF = 0.8\n",
        "MIN_COUNT = 5\n",
        "vocabulary, word_doc_freq = build_vocabulary(train_tokenized, max_doc_freq=MAX_DF, min_count=MIN_COUNT)\n",
        "\n",
        "train_vectors = vectorize_texts(train_tokenized, vocabulary, word_doc_freq, mode=VECTORIZATION_MODE)\n",
        "test_vectors = vectorize_texts(test_tokenized, vocabulary, word_doc_freq, mode=VECTORIZATION_MODE)\n",
        "\n",
        "train_dataset = SparseFeaturesDataset(train_vectors, train_source['target'])\n",
        "test_dataset = SparseFeaturesDataset(test_vectors, test_source['target'])\n",
        "\n",
        "UNIQUE_WORDS_N = len(vocabulary)\n",
        "UNIQUE_LABELS_N = len(set(train_source['target']))\n",
        "model = nn.Linear(UNIQUE_WORDS_N, UNIQUE_LABELS_N)\n",
        "\n",
        "scheduler = lambda optim: torch.optim.lr_scheduler.ReduceLROnPlateau(optim, patience=5, factor=0.5, verbose=True)\n",
        "\n",
        "best_val_loss, best_model = train_eval_loop(model=model,\n",
        "                                            train_dataset=train_dataset,\n",
        "                                            val_dataset=test_dataset,\n",
        "                                            criterion=F.cross_entropy,\n",
        "                                            lr=LR,\n",
        "                                            epoch_n=EPOCH,\n",
        "                                            batch_size=BATCH,\n",
        "                                            l2_reg_alpha=L2,\n",
        "                                            lr_scheduler_ctor=scheduler)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jCrnU8WKr4NV",
        "colab_type": "code",
        "outputId": "0c280a7f-40f1-4fde-b293-84d53c818a73",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "train_pred = predict_with_model(best_model, train_dataset)\n",
        "train_loss = F.cross_entropy(torch.from_numpy(train_pred),torch.from_numpy(train_source['target']).long())\n",
        "test_pred = predict_with_model(best_model, test_dataset)\n",
        "test_loss = F.cross_entropy(torch.from_numpy(test_pred), torch.from_numpy(test_source['target']).long())"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 354/353.5625 [00:01<00:00, 193.50it/s]\n",
            "236it [00:01, 195.72it/s]                             \n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ivvtwx9ZrDgF",
        "colab_type": "code",
        "outputId": "c7ff8e13-5268-461d-b218-4038bac8fe6b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "print('Среднее значение функции потерь на обучении', float(train_loss))\n",
        "print('Доля верных ответов', accuracy_score(train_source['target'], train_pred.argmax(-1)))\n",
        "print()\n",
        "print('Среднее значение функции потерь на валидации', float(test_loss))\n",
        "print('Доля верных ответов', accuracy_score(test_source['target'], test_pred.argmax(-1)))"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Среднее значение функции потерь на обучении 0.0634421557188034\n",
            "Доля верных ответов 0.9934594307937069\n",
            "\n",
            "Среднее значение функции потерь на валидации 3.8096024990081787\n",
            "Доля верных ответов 0.7308815719596389\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MtbnKCZJMki_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "results.append({'mode': 'bin', 'train_accuracy': accuracy_score(train_source['target'], train_pred.argmax(-1)), 'test_accuracy': accuracy_score(test_source['target'], test_pred.argmax(-1)), 'train_loss': float(train_loss), 'test_loss': float(test_loss)})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fFuJoG5MsiMA",
        "colab_type": "text"
      },
      "source": [
        "#VECTORIZATION_MODE: pmi"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nSJXZXLiQsRB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def vectorize_texts_pmi(tokenized_texts, word2id, target):\n",
        "    word_count_per_label = scipy.sparse.dok_matrix((len(set(target)), len(word2id)), dtype='float32')\n",
        "    for text_i, text in enumerate(tokenized_texts):\n",
        "        for token in text:\n",
        "            if token in word2id:\n",
        "                word_count_per_label[target[text_i], word2id[token]] += 1\n",
        "\n",
        "    _, counts_target = np.unique(target, return_counts=True)\n",
        "    pwl = word_count_per_label / word_count_per_label.sum()\n",
        "    pw = word_count_per_label.sum(0) / word_count_per_label.sum()\n",
        "    pl = counts_target / counts_target.sum()\n",
        "    \n",
        "    pmi = scipy.sparse.dok_matrix((len(tokenized_texts), len(word2id)), dtype='float32')\n",
        "    for text_i, text in enumerate(tokenized_texts):\n",
        "        for token in text:\n",
        "            if token in word2id:\n",
        "                pmi[text_i, word2id[token]] = np.log2(pwl[target[text_i], word2id[token]] / (pw[0, word2id[token]] * pl[target[text_i]]))\n",
        "      \n",
        "    return scipy.sparse.csr_matrix(pmi)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "liEe9XdO9O_s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_source = fetch_20newsgroups(subset='train')\n",
        "test_source = fetch_20newsgroups(subset='test')\n",
        "\n",
        "train_tokenized = tokenize_corpus(train_source['data'])\n",
        "test_tokenized = tokenize_corpus(test_source['data'])\n",
        "\n",
        "MAX_DF = 0.8\n",
        "MIN_COUNT = 5\n",
        "vocabulary, word_doc_freq = build_vocabulary(train_tokenized, max_doc_freq=MAX_DF, min_count=MIN_COUNT)\n",
        "\n",
        "train_vectors = vectorize_texts_pmi(train_tokenized, vocabulary, train_source['target'])\n",
        "test_vectors = vectorize_texts_pmi(test_tokenized, vocabulary, test_source['target'])\n",
        "\n",
        "train_dataset = SparseFeaturesDataset(train_vectors, train_source['target'])\n",
        "test_dataset = SparseFeaturesDataset(test_vectors, test_source['target'])\n",
        "\n",
        "UNIQUE_WORDS_N = len(vocabulary)\n",
        "UNIQUE_LABELS_N = len(set(train_source['target']))\n",
        "model = nn.Linear(UNIQUE_WORDS_N, UNIQUE_LABELS_N)\n",
        "\n",
        "scheduler = lambda optim: torch.optim.lr_scheduler.ReduceLROnPlateau(optim, patience=5, factor=0.5, verbose=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LKXFMB_b-iNz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "LR = 1e-1\n",
        "L2 = 1e-3\n",
        "BATCH = 32\n",
        "EPOCH = 200"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rkR7Py9Se0bm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "best_val_loss, best_model = train_eval_loop(model=model,\n",
        "                                            train_dataset=train_dataset,\n",
        "                                            val_dataset=test_dataset,\n",
        "                                            criterion=F.cross_entropy,\n",
        "                                            lr=LR,\n",
        "                                            epoch_n=EPOCH,\n",
        "                                            batch_size=BATCH,\n",
        "                                            l2_reg_alpha=L2,\n",
        "                                            lr_scheduler_ctor=scheduler)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "otRHujSAe0Yh",
        "colab_type": "code",
        "outputId": "1e20ad90-cd5e-46c8-eb09-5ea69c0d4dea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "train_pred = predict_with_model(best_model, train_dataset)\n",
        "train_loss = F.cross_entropy(torch.from_numpy(train_pred),torch.from_numpy(train_source['target']).long())\n",
        "test_pred = predict_with_model(best_model, test_dataset)\n",
        "test_loss = F.cross_entropy(torch.from_numpy(test_pred), torch.from_numpy(test_source['target']).long())"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 354/353.5625 [00:01<00:00, 178.81it/s]\n",
            "236it [00:01, 177.61it/s]                             \n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "miAk1GiXe0WR",
        "colab_type": "code",
        "outputId": "a452c78f-7ed7-4648-8c70-2e4b1bc5993d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "print('Среднее значение функции потерь на обучении', float(train_loss))\n",
        "print('Доля верных ответов', accuracy_score(train_source['target'], train_pred.argmax(-1)))\n",
        "print()\n",
        "print('Среднее значение функции потерь на валидации', float(test_loss))\n",
        "print('Доля верных ответов', accuracy_score(test_source['target'], test_pred.argmax(-1)))"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Среднее значение функции потерь на обучении 0.05344357341527939\n",
            "Доля верных ответов 0.9992045253668022\n",
            "\n",
            "Среднее значение функции потерь на валидации 0.317936509847641\n",
            "Доля верных ответов 0.9358736059479554\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yEePSPi2CmLj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "results.append({'mode': 'pmi', 'train_accuracy': accuracy_score(train_source['target'], train_pred.argmax(-1)), 'test_accuracy': accuracy_score(test_source['target'], test_pred.argmax(-1)), 'train_loss': float(train_loss), 'test_loss': float(test_loss)})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m5b2MtZA37KJ",
        "colab_type": "text"
      },
      "source": [
        "#VECTORIZATION_MODE: lsa"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HF56XQsE2vy2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def vectorize_texts_lsa(tokenized_texts_train, tokenized_texts_test, word2id, n_components=100, scale=True):\n",
        "\n",
        "    train_word_count = scipy.sparse.dok_matrix((len(tokenized_texts_train), len(word2id)), dtype='float32')\n",
        "    for text_i, text in enumerate(tokenized_texts_train):\n",
        "        for token in text:\n",
        "            if token in word2id:\n",
        "                train_word_count[text_i, word2id[token]] += 1\n",
        "\n",
        "    test_word_count = scipy.sparse.dok_matrix((len(tokenized_texts_test), len(word2id)), dtype='float32')\n",
        "    for text_i, text in enumerate(tokenized_texts_test):\n",
        "        for token in text:\n",
        "            if token in word2id:\n",
        "                test_word_count[text_i, word2id[token]] += 1\n",
        "\n",
        "    lsa = sklearn.decomposition.TruncatedSVD(n_components)\n",
        "    train_vectors = lsa.fit_transform(train_word_count)\n",
        "    test_vectors = lsa.transform(test_word_count)\n",
        "\n",
        "    if scale:\n",
        "        norm = sklearn.preprocessing.Normalizer(copy=False)\n",
        "        train_vectors = norm.fit_transform(train_vectors)\n",
        "        test_vectors = norm.transform(test_vectors)\n",
        "\n",
        "    return scipy.sparse.csr_matrix(train_vectors), scipy.sparse.csr_matrix(test_vectors)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DN5ci3qItNbr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_source = fetch_20newsgroups(subset='train')\n",
        "test_source = fetch_20newsgroups(subset='test')\n",
        "\n",
        "train_tokenized = tokenize_corpus(train_source['data'])\n",
        "test_tokenized = tokenize_corpus(test_source['data'])\n",
        "\n",
        "MAX_DF = 0.8\n",
        "MIN_COUNT = 5\n",
        "vocabulary, word_doc_freq = build_vocabulary(train_tokenized, max_doc_freq=MAX_DF, min_count=MIN_COUNT)\n",
        "\n",
        "train_vectors, test_vectors = vectorize_texts_lsa(train_tokenized, test_tokenized, vocabulary)\n",
        "\n",
        "train_dataset = SparseFeaturesDataset(train_vectors, train_source['target'])\n",
        "test_dataset = SparseFeaturesDataset(test_vectors, test_source['target'])\n",
        "\n",
        "UNIQUE_WORDS_N = train_vectors.shape[1]\n",
        "UNIQUE_LABELS_N = len(set(train_source['target']))\n",
        "model = nn.Linear(UNIQUE_WORDS_N, UNIQUE_LABELS_N)\n",
        "\n",
        "scheduler = lambda optim: torch.optim.lr_scheduler.ReduceLROnPlateau(optim, patience=5, factor=0.5, verbose=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vi2VpRyhxEmu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "LR = 1e-1\n",
        "L2 = 0\n",
        "BATCH = 32\n",
        "EPOCH = 200"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r61Z5H1_xEsd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "best_val_loss, best_model = train_eval_loop(model=model,\n",
        "                                            train_dataset=train_dataset,\n",
        "                                            val_dataset=test_dataset,\n",
        "                                            criterion=F.cross_entropy,\n",
        "                                            lr=LR,\n",
        "                                            epoch_n=EPOCH,\n",
        "                                            batch_size=BATCH,\n",
        "                                            l2_reg_alpha=L2,\n",
        "                                            lr_scheduler_ctor=scheduler)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eMC5FcYrxEqB",
        "colab_type": "code",
        "outputId": "eb6e2afa-6bd6-4901-bc5c-95ab60f0d0c6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "train_pred = predict_with_model(best_model, train_dataset)\n",
        "train_loss = F.cross_entropy(torch.from_numpy(train_pred),torch.from_numpy(train_source['target']).long())\n",
        "test_pred = predict_with_model(best_model, test_dataset)\n",
        "test_loss = F.cross_entropy(torch.from_numpy(test_pred), torch.from_numpy(test_source['target']).long())"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 354/353.5625 [00:01<00:00, 271.31it/s]\n",
            "236it [00:00, 281.45it/s]                             \n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2CBERrRy09rv",
        "colab_type": "code",
        "outputId": "10355160-43cf-4f11-84bd-43f2aa2814c4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "print('Среднее значение функции потерь на обучении', float(train_loss))\n",
        "print('Доля верных ответов', accuracy_score(train_source['target'], train_pred.argmax(-1)))\n",
        "print()\n",
        "print('Среднее значение функции потерь на валидации', float(test_loss))\n",
        "print('Доля верных ответов', accuracy_score(test_source['target'], test_pred.argmax(-1)))"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Среднее значение функции потерь на обучении 0.9047325849533081\n",
            "Доля верных ответов 0.72379353013965\n",
            "\n",
            "Среднее значение функции потерь на валидации 1.313632845878601\n",
            "Доля верных ответов 0.6072756240042485\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mr1oapS009zy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "results.append({'mode': 'lsa', 'train_accuracy': accuracy_score(train_source['target'], train_pred.argmax(-1)), 'test_accuracy': accuracy_score(test_source['target'], test_pred.argmax(-1)), 'train_loss': float(train_loss), 'test_loss': float(test_loss)})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gmUYJFWk4cF5",
        "colab_type": "text"
      },
      "source": [
        "# RESULTS: top vectorization mode"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GDTRiP3_zC8C",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "outputId": "ed05e148-2bcb-4e1c-9746-974460cd59c2"
      },
      "source": [
        "pd.DataFrame(results).sort_values(by=['test_accuracy'], ascending=False)"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>mode</th>\n",
              "      <th>train_accuracy</th>\n",
              "      <th>test_accuracy</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>test_loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>pmi</td>\n",
              "      <td>0.999205</td>\n",
              "      <td>0.935874</td>\n",
              "      <td>0.053444</td>\n",
              "      <td>0.317937</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>tf</td>\n",
              "      <td>0.999205</td>\n",
              "      <td>0.814525</td>\n",
              "      <td>0.005827</td>\n",
              "      <td>0.656580</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>tfidf</td>\n",
              "      <td>0.999293</td>\n",
              "      <td>0.771243</td>\n",
              "      <td>0.004175</td>\n",
              "      <td>0.928882</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>idf</td>\n",
              "      <td>0.998409</td>\n",
              "      <td>0.752523</td>\n",
              "      <td>0.021615</td>\n",
              "      <td>1.002797</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>bin</td>\n",
              "      <td>0.993459</td>\n",
              "      <td>0.730882</td>\n",
              "      <td>0.063442</td>\n",
              "      <td>3.809602</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>lsa</td>\n",
              "      <td>0.723794</td>\n",
              "      <td>0.607276</td>\n",
              "      <td>0.904733</td>\n",
              "      <td>1.313633</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    mode  train_accuracy  test_accuracy  train_loss  test_loss\n",
              "5    pmi        0.999205       0.935874    0.053444   0.317937\n",
              "1     tf        0.999205       0.814525    0.005827   0.656580\n",
              "0  tfidf        0.999293       0.771243    0.004175   0.928882\n",
              "2    idf        0.998409       0.752523    0.021615   1.002797\n",
              "3    bin        0.993459       0.730882    0.063442   3.809602\n",
              "4    lsa        0.723794       0.607276    0.904733   1.313633"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4heMM-BU8auz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rVZ2vXRi9XIc",
        "colab_type": "text"
      },
      "source": [
        "#STEMMING"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MAJpPZzi9Wbs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import nltk\n",
        "# nltk.download('wordnet')\n",
        "sno = nltk.stem.SnowballStemmer('english')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JfHYjvz19ccU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "results1 = []"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uKcvnEmi8u7A",
        "colab_type": "text"
      },
      "source": [
        "#STEMMING: VECTORIZATION_MODE: pmi"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gsKs4U11tNN6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_source = fetch_20newsgroups(subset='train')\n",
        "test_source = fetch_20newsgroups(subset='test')\n",
        "\n",
        "train_tokenized = tokenize_corpus(train_source['data'])\n",
        "test_tokenized = tokenize_corpus(test_source['data'])\n",
        "\n",
        "train_tokenized = [[sno.stem(word) for word in doc] for doc in train_tokenized]\n",
        "test_tokenized = [[sno.stem(word) for word in doc] for doc in test_tokenized]\n",
        "\n",
        "MAX_DF = 0.8\n",
        "MIN_COUNT = 5\n",
        "vocabulary, word_doc_freq = build_vocabulary(train_tokenized, max_doc_freq=MAX_DF, min_count=MIN_COUNT)\n",
        "\n",
        "train_vectors = vectorize_texts_pmi(train_tokenized, vocabulary, train_source['target'])\n",
        "test_vectors = vectorize_texts_pmi(test_tokenized, vocabulary, test_source['target'])\n",
        "\n",
        "train_dataset = SparseFeaturesDataset(train_vectors, train_source['target'])\n",
        "test_dataset = SparseFeaturesDataset(test_vectors, test_source['target'])\n",
        "\n",
        "UNIQUE_WORDS_N = len(vocabulary)\n",
        "UNIQUE_LABELS_N = len(set(train_source['target']))\n",
        "model = nn.Linear(UNIQUE_WORDS_N, UNIQUE_LABELS_N)\n",
        "\n",
        "scheduler = lambda optim: torch.optim.lr_scheduler.ReduceLROnPlateau(optim, patience=5, factor=0.5, verbose=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nmgv85NZ7vXR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "LR = 1e-1\n",
        "L2 = 1e-3\n",
        "BATCH = 32\n",
        "EPOCH = 200"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oE6lrmrk7ven",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "best_val_loss, best_model = train_eval_loop(model=model,\n",
        "                                            train_dataset=train_dataset,\n",
        "                                            val_dataset=test_dataset,\n",
        "                                            criterion=F.cross_entropy,\n",
        "                                            lr=LR,\n",
        "                                            epoch_n=EPOCH,\n",
        "                                            batch_size=BATCH,\n",
        "                                            l2_reg_alpha=L2,\n",
        "                                            lr_scheduler_ctor=scheduler)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "igVyNY227vck",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "db6669fb-092e-43fb-b5ea-464ded697fa3"
      },
      "source": [
        "train_pred = predict_with_model(best_model, train_dataset)\n",
        "train_loss = F.cross_entropy(torch.from_numpy(train_pred),torch.from_numpy(train_source['target']).long())\n",
        "test_pred = predict_with_model(best_model, test_dataset)\n",
        "test_loss = F.cross_entropy(torch.from_numpy(test_pred), torch.from_numpy(test_source['target']).long())"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 354/353.5625 [00:01<00:00, 202.29it/s]\n",
            "236it [00:01, 206.04it/s]                             \n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Hk-ZAC98JC3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "792e5f4b-dd9d-4075-a709-b751b685c585"
      },
      "source": [
        "print('Среднее значение функции потерь на обучении', float(train_loss))\n",
        "print('Доля верных ответов', accuracy_score(train_source['target'], train_pred.argmax(-1)))\n",
        "print()\n",
        "print('Среднее значение функции потерь на валидации', float(test_loss))\n",
        "print('Доля верных ответов', accuracy_score(test_source['target'], test_pred.argmax(-1)))"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Среднее значение функции потерь на обучении 0.05469982698559761\n",
            "Доля верных ответов 0.9993812975075128\n",
            "\n",
            "Среднее значение функции потерь на валидации 0.3036949932575226\n",
            "Доля верных ответов 0.942777482740308\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FayJkm-48JKk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "results1.append({'mode': 'pmi', 'train_accuracy': accuracy_score(train_source['target'], train_pred.argmax(-1)), 'test_accuracy': accuracy_score(test_source['target'], test_pred.argmax(-1)), 'train_loss': float(train_loss), 'test_loss': float(test_loss)})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WsrdnLrfDQ8G",
        "colab_type": "text"
      },
      "source": [
        "#STEMMING: VECTORIZATION_MODE: tf"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RE8PP7cj8JIH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "VECTORIZATION_MODE = 'tf'\n",
        "LR = 1e-1\n",
        "L2 = 0\n",
        "BATCH = 32\n",
        "EPOCH = 200"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FyGeCLuc8JGG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_source = fetch_20newsgroups(subset='train')\n",
        "test_source = fetch_20newsgroups(subset='test')\n",
        "\n",
        "train_tokenized = tokenize_corpus(train_source['data'])\n",
        "test_tokenized = tokenize_corpus(test_source['data'])\n",
        "\n",
        "train_tokenized = [[sno.stem(word) for word in doc] for doc in train_tokenized]\n",
        "test_tokenized = [[sno.stem(word) for word in doc] for doc in test_tokenized]\n",
        "\n",
        "MAX_DF = 0.8\n",
        "MIN_COUNT = 5\n",
        "vocabulary, word_doc_freq = build_vocabulary(train_tokenized, max_doc_freq=MAX_DF, min_count=MIN_COUNT)\n",
        "\n",
        "train_vectors = vectorize_texts(train_tokenized, vocabulary, word_doc_freq, mode=VECTORIZATION_MODE)\n",
        "test_vectors = vectorize_texts(test_tokenized, vocabulary, word_doc_freq, mode=VECTORIZATION_MODE)\n",
        "\n",
        "train_dataset = SparseFeaturesDataset(train_vectors, train_source['target'])\n",
        "test_dataset = SparseFeaturesDataset(test_vectors, test_source['target'])\n",
        "\n",
        "UNIQUE_WORDS_N = len(vocabulary)\n",
        "UNIQUE_LABELS_N = len(set(train_source['target']))\n",
        "model = nn.Linear(UNIQUE_WORDS_N, UNIQUE_LABELS_N)\n",
        "\n",
        "scheduler = lambda optim: torch.optim.lr_scheduler.ReduceLROnPlateau(optim, patience=5, factor=0.5, verbose=True)\n",
        "\n",
        "best_val_loss, best_model = train_eval_loop(model=model,\n",
        "                                            train_dataset=train_dataset,\n",
        "                                            val_dataset=test_dataset,\n",
        "                                            criterion=F.cross_entropy,\n",
        "                                            lr=LR,\n",
        "                                            epoch_n=EPOCH,\n",
        "                                            batch_size=BATCH,\n",
        "                                            l2_reg_alpha=L2,\n",
        "                                            lr_scheduler_ctor=scheduler)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hw75IcK67vae",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "859b08fe-7f45-44fc-e320-b28542b8fca0"
      },
      "source": [
        "train_pred = predict_with_model(best_model, train_dataset)\n",
        "train_loss = F.cross_entropy(torch.from_numpy(train_pred),torch.from_numpy(train_source['target']).long())\n",
        "test_pred = predict_with_model(best_model, test_dataset)\n",
        "test_loss = F.cross_entropy(torch.from_numpy(test_pred), torch.from_numpy(test_source['target']).long())"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 354/353.5625 [00:01<00:00, 189.97it/s]\n",
            "236it [00:01, 186.77it/s]                             \n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j4QE_JnkDZsE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "114d7ccc-e3ac-4656-9d79-a21aba3033e3"
      },
      "source": [
        "print('Среднее значение функции потерь на обучении', float(train_loss))\n",
        "print('Доля верных ответов', accuracy_score(train_source['target'], train_pred.argmax(-1)))\n",
        "print()\n",
        "print('Среднее значение функции потерь на валидации', float(test_loss))\n",
        "print('Доля верных ответов', accuracy_score(test_source['target'], test_pred.argmax(-1)))"
      ],
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Среднее значение функции потерь на обучении 0.010306037962436676\n",
            "Доля верных ответов 0.9992045253668022\n",
            "\n",
            "Среднее значение функции потерь на валидации 0.6652761101722717\n",
            "Доля верных ответов 0.8113382899628253\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kCrfz7RkDZ5e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "results1.append({'mode': 'tf', 'train_accuracy': accuracy_score(train_source['target'], train_pred.argmax(-1)), 'test_accuracy': accuracy_score(test_source['target'], test_pred.argmax(-1)), 'train_loss': float(train_loss), 'test_loss': float(test_loss)})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LfXPTmecDx0z",
        "colab_type": "text"
      },
      "source": [
        "#STEMMING: VECTORIZATION_MODE: tfidf"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2uY-9DIHDZ2_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "VECTORIZATION_MODE = 'tfidf'\n",
        "LR = 1e-1\n",
        "L2 = 0\n",
        "BATCH = 32\n",
        "EPOCH = 200"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ISPPPlT8D1LE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_source = fetch_20newsgroups(subset='train')\n",
        "test_source = fetch_20newsgroups(subset='test')\n",
        "\n",
        "train_tokenized = tokenize_corpus(train_source['data'])\n",
        "test_tokenized = tokenize_corpus(test_source['data'])\n",
        "\n",
        "train_tokenized = [[sno.stem(word) for word in doc] for doc in train_tokenized]\n",
        "test_tokenized = [[sno.stem(word) for word in doc] for doc in test_tokenized]\n",
        "\n",
        "MAX_DF = 0.8\n",
        "MIN_COUNT = 5\n",
        "vocabulary, word_doc_freq = build_vocabulary(train_tokenized, max_doc_freq=MAX_DF, min_count=MIN_COUNT)\n",
        "\n",
        "train_vectors = vectorize_texts(train_tokenized, vocabulary, word_doc_freq, mode=VECTORIZATION_MODE)\n",
        "test_vectors = vectorize_texts(test_tokenized, vocabulary, word_doc_freq, mode=VECTORIZATION_MODE)\n",
        "\n",
        "train_dataset = SparseFeaturesDataset(train_vectors, train_source['target'])\n",
        "test_dataset = SparseFeaturesDataset(test_vectors, test_source['target'])\n",
        "\n",
        "UNIQUE_WORDS_N = len(vocabulary)\n",
        "UNIQUE_LABELS_N = len(set(train_source['target']))\n",
        "model = nn.Linear(UNIQUE_WORDS_N, UNIQUE_LABELS_N)\n",
        "\n",
        "scheduler = lambda optim: torch.optim.lr_scheduler.ReduceLROnPlateau(optim, patience=5, factor=0.5, verbose=True)\n",
        "\n",
        "best_val_loss, best_model = train_eval_loop(model=model,\n",
        "                                            train_dataset=train_dataset,\n",
        "                                            val_dataset=test_dataset,\n",
        "                                            criterion=F.cross_entropy,\n",
        "                                            lr=LR,\n",
        "                                            epoch_n=EPOCH,\n",
        "                                            batch_size=BATCH,\n",
        "                                            l2_reg_alpha=L2,\n",
        "                                            lr_scheduler_ctor=scheduler)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LFPq1-NADZ0g",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "996a21cb-71fe-4251-b94c-d7ea7192d3dc"
      },
      "source": [
        "train_pred = predict_with_model(best_model, train_dataset)\n",
        "train_loss = F.cross_entropy(torch.from_numpy(train_pred),torch.from_numpy(train_source['target']).long())\n",
        "test_pred = predict_with_model(best_model, test_dataset)\n",
        "test_loss = F.cross_entropy(torch.from_numpy(test_pred), torch.from_numpy(test_source['target']).long())"
      ],
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 354/353.5625 [00:01<00:00, 203.15it/s]\n",
            "236it [00:01, 208.02it/s]                             \n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mRmlAokODZyX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "fd8899c7-9726-49c0-c5f5-fbedbfb6e001"
      },
      "source": [
        "print('Среднее значение функции потерь на обучении', float(train_loss))\n",
        "print('Доля верных ответов', accuracy_score(train_source['target'], train_pred.argmax(-1)))\n",
        "print()\n",
        "print('Среднее значение функции потерь на валидации', float(test_loss))\n",
        "print('Доля верных ответов', accuracy_score(test_source['target'], test_pred.argmax(-1)))"
      ],
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Среднее значение функции потерь на обучении 0.013925576582551003\n",
            "Доля верных ответов 0.9991161392964468\n",
            "\n",
            "Среднее значение функции потерь на валидации 0.9857187271118164\n",
            "Доля верных ответов 0.7636749867233139\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jnKSUVrWDZvu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "results1.append({'mode': 'tfidf', 'train_accuracy': accuracy_score(train_source['target'], train_pred.argmax(-1)), 'test_accuracy': accuracy_score(test_source['target'], test_pred.argmax(-1)), 'train_loss': float(train_loss), 'test_loss': float(test_loss)})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6oOPZou3EMxd",
        "colab_type": "text"
      },
      "source": [
        "# RESULTS: top stemming"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZVovopuWEDj5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        },
        "outputId": "eb7ea955-d838-46bc-8619-f5fba4865f3a"
      },
      "source": [
        "pd.DataFrame(results1).sort_values(by=['test_accuracy'], ascending=False)"
      ],
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>mode</th>\n",
              "      <th>train_accuracy</th>\n",
              "      <th>test_accuracy</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>test_loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>pmi</td>\n",
              "      <td>0.999381</td>\n",
              "      <td>0.942777</td>\n",
              "      <td>0.054700</td>\n",
              "      <td>0.303695</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>tf</td>\n",
              "      <td>0.999205</td>\n",
              "      <td>0.811338</td>\n",
              "      <td>0.010306</td>\n",
              "      <td>0.665276</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>tfidf</td>\n",
              "      <td>0.999116</td>\n",
              "      <td>0.763675</td>\n",
              "      <td>0.013926</td>\n",
              "      <td>0.985719</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    mode  train_accuracy  test_accuracy  train_loss  test_loss\n",
              "0    pmi        0.999381       0.942777    0.054700   0.303695\n",
              "1     tf        0.999205       0.811338    0.010306   0.665276\n",
              "2  tfidf        0.999116       0.763675    0.013926   0.985719"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 98
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EqK7l0mgHz-F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nO0tWA8XEovT",
        "colab_type": "text"
      },
      "source": [
        "#N-gram (word 2-gram)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MlCnaLmBEDs-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "result2 = []"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ObctnYo3FqaI",
        "colab_type": "text"
      },
      "source": [
        "#N-gram: VECTORIZATION_MODE: pmi"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lfk-_RugGl5Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_source = fetch_20newsgroups(subset='train')\n",
        "test_source = fetch_20newsgroups(subset='test')\n",
        "\n",
        "train_tokenized = tokenize_corpus(train_source['data'])\n",
        "test_tokenized = tokenize_corpus(test_source['data'])\n",
        "\n",
        "train_tokenized = [[f\"{doc[i_word]}_{doc[i_word+1]}\" for i_word in range(len(doc)-1)] for doc in train_tokenized]\n",
        "test_tokenized = [[f\"{doc[i_word]}_{doc[i_word+1]}\" for i_word in range(len(doc)-1)] for doc in test_tokenized]\n",
        "\n",
        "MAX_DF = 0.8\n",
        "MIN_COUNT = 5\n",
        "vocabulary, word_doc_freq = build_vocabulary(train_tokenized, max_doc_freq=MAX_DF, min_count=MIN_COUNT)\n",
        "\n",
        "train_vectors = vectorize_texts_pmi(train_tokenized, vocabulary, train_source['target'])\n",
        "test_vectors = vectorize_texts_pmi(test_tokenized, vocabulary, test_source['target'])\n",
        "\n",
        "train_dataset = SparseFeaturesDataset(train_vectors, train_source['target'])\n",
        "test_dataset = SparseFeaturesDataset(test_vectors, test_source['target'])\n",
        "\n",
        "UNIQUE_WORDS_N = len(vocabulary)\n",
        "UNIQUE_LABELS_N = len(set(train_source['target']))\n",
        "model = nn.Linear(UNIQUE_WORDS_N, UNIQUE_LABELS_N)\n",
        "\n",
        "scheduler = lambda optim: torch.optim.lr_scheduler.ReduceLROnPlateau(optim, patience=5, factor=0.5, verbose=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BmpYk_q_GmEH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "LR = 1e-1\n",
        "L2 = 1e-3\n",
        "BATCH = 32\n",
        "EPOCH = 200"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vq4QeKtRGmBA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "42f110a8-184e-4d71-c69a-d4683c5a47f3"
      },
      "source": [
        "best_val_loss, best_model = train_eval_loop(model=model,\n",
        "                                            train_dataset=train_dataset,\n",
        "                                            val_dataset=test_dataset,\n",
        "                                            criterion=F.cross_entropy,\n",
        "                                            lr=LR,\n",
        "                                            epoch_n=EPOCH,\n",
        "                                            batch_size=BATCH,\n",
        "                                            l2_reg_alpha=L2,\n",
        "                                            lr_scheduler_ctor=scheduler)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Эпоха 0\n",
            "Эпоха: 354 итераций, 7.10 сек\n",
            "Среднее значение функции потерь на обучении 1.0902491315862552\n",
            "Среднее значение функции потерь на валидации 2.6270006993564508\n",
            "Новая лучшая модель!\n",
            "\n",
            "Эпоха 1\n",
            "Эпоха: 354 итераций, 6.76 сек\n",
            "Среднее значение функции потерь на обучении 1.1059183079517831\n",
            "Среднее значение функции потерь на валидации 2.315061278767505\n",
            "Новая лучшая модель!\n",
            "\n",
            "Эпоха 2\n",
            "Эпоха: 354 итераций, 7.07 сек\n",
            "Среднее значение функции потерь на обучении 1.1590289650958474\n",
            "Среднее значение функции потерь на валидации 2.2983832215353592\n",
            "Новая лучшая модель!\n",
            "\n",
            "Эпоха 3\n",
            "Эпоха: 354 итераций, 6.84 сек\n",
            "Среднее значение функции потерь на обучении 1.0675788229215617\n",
            "Среднее значение функции потерь на валидации 2.729440582505727\n",
            "\n",
            "Эпоха 4\n",
            "Эпоха: 354 итераций, 6.82 сек\n",
            "Среднее значение функции потерь на обучении 1.1533394707735143\n",
            "Среднее значение функции потерь на валидации 2.5741452092336394\n",
            "\n",
            "Эпоха 5\n",
            "Эпоха: 354 итераций, 6.79 сек\n",
            "Среднее значение функции потерь на обучении 1.1349304500904123\n",
            "Среднее значение функции потерь на валидации 2.7931036486969156\n",
            "\n",
            "Эпоха 6\n",
            "Эпоха: 354 итераций, 6.77 сек\n",
            "Среднее значение функции потерь на обучении 1.1131107284987376\n",
            "Среднее значение функции потерь на валидации 2.5101750924930735\n",
            "\n",
            "Эпоха 7\n",
            "Эпоха: 354 итераций, 6.76 сек\n",
            "Среднее значение функции потерь на обучении 1.0853723278742726\n",
            "Среднее значение функции потерь на валидации 2.6665362536402073\n",
            "\n",
            "Эпоха 8\n",
            "Эпоха: 354 итераций, 6.64 сек\n",
            "Среднее значение функции потерь на обучении 1.2756028779819186\n",
            "Среднее значение функции потерь на валидации 2.4246493992158924\n",
            "Epoch     9: reducing learning rate of group 0 to 5.0000e-02.\n",
            "\n",
            "Эпоха 9\n",
            "Эпоха: 354 итераций, 6.69 сек\n",
            "Среднее значение функции потерь на обучении 0.5881191465456439\n",
            "Среднее значение функции потерь на валидации 1.7000715452230584\n",
            "Новая лучшая модель!\n",
            "\n",
            "Эпоха 10\n",
            "Эпоха: 354 итераций, 6.86 сек\n",
            "Среднее значение функции потерь на обучении 0.5274892526199926\n",
            "Среднее значение функции потерь на валидации 1.7455104238401025\n",
            "\n",
            "Эпоха 11\n",
            "Эпоха: 354 итераций, 6.87 сек\n",
            "Среднее значение функции потерь на обучении 0.6131722815334797\n",
            "Среднее значение функции потерь на валидации 1.5682609533354388\n",
            "Новая лучшая модель!\n",
            "\n",
            "Эпоха 12\n",
            "Эпоха: 354 итераций, 6.71 сек\n",
            "Среднее значение функции потерь на обучении 0.6303199280453267\n",
            "Среднее значение функции потерь на валидации 1.726655695397975\n",
            "\n",
            "Эпоха 13\n",
            "Эпоха: 354 итераций, 6.66 сек\n",
            "Среднее значение функции потерь на обучении 0.5849938880925798\n",
            "Среднее значение функции потерь на валидации 1.6602301716299381\n",
            "\n",
            "Эпоха 14\n",
            "Эпоха: 354 итераций, 6.67 сек\n",
            "Среднее значение функции потерь на обучении 0.6127991417875398\n",
            "Среднее значение функции потерь на валидации 1.6501374764967773\n",
            "\n",
            "Эпоха 15\n",
            "Эпоха: 354 итераций, 6.80 сек\n",
            "Среднее значение функции потерь на обучении 0.6314706531745292\n",
            "Среднее значение функции потерь на валидации 1.6214403100943162\n",
            "\n",
            "Эпоха 16\n",
            "Эпоха: 354 итераций, 6.71 сек\n",
            "Среднее значение функции потерь на обучении 0.5867870398876021\n",
            "Среднее значение функции потерь на валидации 1.859982948434555\n",
            "\n",
            "Эпоха 17\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CSiE2mENGtw6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_pred = predict_with_model(best_model, train_dataset)\n",
        "train_loss = F.cross_entropy(torch.from_numpy(train_pred),torch.from_numpy(train_source['target']).long())\n",
        "test_pred = predict_with_model(best_model, test_dataset)\n",
        "test_loss = F.cross_entropy(torch.from_numpy(test_pred), torch.from_numpy(test_source['target']).long())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a439WphQGt3-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print('Среднее значение функции потерь на обучении', float(train_loss))\n",
        "print('Доля верных ответов', accuracy_score(train_source['target'], train_pred.argmax(-1)))\n",
        "print()\n",
        "print('Среднее значение функции потерь на валидации', float(test_loss))\n",
        "print('Доля верных ответов', accuracy_score(test_source['target'], test_pred.argmax(-1)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BfaeVMaaGl9o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "results2.append({'mode': 'pmi', 'train_accuracy': accuracy_score(train_source['target'], train_pred.argmax(-1)), 'test_accuracy': accuracy_score(test_source['target'], test_pred.argmax(-1)), 'train_loss': float(train_loss), 'test_loss': float(test_loss)})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-nZGhrJNGc2h",
        "colab_type": "text"
      },
      "source": [
        "#N-gram: VECTORIZATION_MODE: tf"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lzgPYu-QGmjm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "VECTORIZATION_MODE = 'tf'\n",
        "LR = 1e-1\n",
        "L2 = 0\n",
        "BATCH = 32\n",
        "EPOCH = 200"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yRI5SBlPGmpr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_source = fetch_20newsgroups(subset='train')\n",
        "test_source = fetch_20newsgroups(subset='test')\n",
        "\n",
        "train_tokenized = tokenize_corpus(train_source['data'])\n",
        "test_tokenized = tokenize_corpus(test_source['data'])\n",
        "\n",
        "train_tokenized = [[f\"{doc[i_word]}_{doc[i_word+1]}\" for i_word in range(len(doc)-1)] for doc in train_tokenized]\n",
        "test_tokenized = [[f\"{doc[i_word]}_{doc[i_word+1]}\" for i_word in range(len(doc)-1)] for doc in test_tokenized]\n",
        "\n",
        "MAX_DF = 0.8\n",
        "MIN_COUNT = 5\n",
        "vocabulary, word_doc_freq = build_vocabulary(train_tokenized, max_doc_freq=MAX_DF, min_count=MIN_COUNT)\n",
        "\n",
        "train_vectors = vectorize_texts(train_tokenized, vocabulary, word_doc_freq, mode=VECTORIZATION_MODE)\n",
        "test_vectors = vectorize_texts(test_tokenized, vocabulary, word_doc_freq, mode=VECTORIZATION_MODE)\n",
        "\n",
        "train_dataset = SparseFeaturesDataset(train_vectors, train_source['target'])\n",
        "test_dataset = SparseFeaturesDataset(test_vectors, test_source['target'])\n",
        "\n",
        "UNIQUE_WORDS_N = len(vocabulary)\n",
        "UNIQUE_LABELS_N = len(set(train_source['target']))\n",
        "model = nn.Linear(UNIQUE_WORDS_N, UNIQUE_LABELS_N)\n",
        "\n",
        "scheduler = lambda optim: torch.optim.lr_scheduler.ReduceLROnPlateau(optim, patience=5, factor=0.5, verbose=True)\n",
        "\n",
        "best_val_loss, best_model = train_eval_loop(model=model,\n",
        "                                            train_dataset=train_dataset,\n",
        "                                            val_dataset=test_dataset,\n",
        "                                            criterion=F.cross_entropy,\n",
        "                                            lr=LR,\n",
        "                                            epoch_n=EPOCH,\n",
        "                                            batch_size=BATCH,\n",
        "                                            l2_reg_alpha=L2,\n",
        "                                            lr_scheduler_ctor=scheduler)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "09mlvvexGmuv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_pred = predict_with_model(best_model, train_dataset)\n",
        "train_loss = F.cross_entropy(torch.from_numpy(train_pred),torch.from_numpy(train_source['target']).long())\n",
        "test_pred = predict_with_model(best_model, test_dataset)\n",
        "test_loss = F.cross_entropy(torch.from_numpy(test_pred), torch.from_numpy(test_source['target']).long())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OeL44FmjHAOM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print('Среднее значение функции потерь на обучении', float(train_loss))\n",
        "print('Доля верных ответов', accuracy_score(train_source['target'], train_pred.argmax(-1)))\n",
        "print()\n",
        "print('Среднее значение функции потерь на валидации', float(test_loss))\n",
        "print('Доля верных ответов', accuracy_score(test_source['target'], test_pred.argmax(-1)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P6-Alb5EGmzU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "results2.append({'mode': 'tf', 'train_accuracy': accuracy_score(train_source['target'], train_pred.argmax(-1)), 'test_accuracy': accuracy_score(test_source['target'], test_pred.argmax(-1)), 'train_loss': float(train_loss), 'test_loss': float(test_loss)})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y6y9aluGGdAr",
        "colab_type": "text"
      },
      "source": [
        "#N-gram: VECTORIZATION_MODE: tfidf"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Azl63RkGnYY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "VECTORIZATION_MODE = 'tfidf'\n",
        "LR = 1e-1\n",
        "L2 = 0\n",
        "BATCH = 32\n",
        "EPOCH = 200"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ho7aIpGQEDyN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_source = fetch_20newsgroups(subset='train')\n",
        "test_source = fetch_20newsgroups(subset='test')\n",
        "\n",
        "train_tokenized = tokenize_corpus(train_source['data'])\n",
        "test_tokenized = tokenize_corpus(test_source['data'])\n",
        "\n",
        "train_tokenized = [[f\"{doc[i_word]}_{doc[i_word+1]}\" for i_word in range(len(doc)-1)] for doc in train_tokenized]\n",
        "test_tokenized = [[f\"{doc[i_word]}_{doc[i_word+1]}\" for i_word in range(len(doc)-1)] for doc in test_tokenized]\n",
        "\n",
        "MAX_DF = 0.8\n",
        "MIN_COUNT = 5\n",
        "vocabulary, word_doc_freq = build_vocabulary(train_tokenized, max_doc_freq=MAX_DF, min_count=MIN_COUNT)\n",
        "\n",
        "train_vectors = vectorize_texts(train_tokenized, vocabulary, word_doc_freq, mode=VECTORIZATION_MODE)\n",
        "test_vectors = vectorize_texts(test_tokenized, vocabulary, word_doc_freq, mode=VECTORIZATION_MODE)\n",
        "\n",
        "train_dataset = SparseFeaturesDataset(train_vectors, train_source['target'])\n",
        "test_dataset = SparseFeaturesDataset(test_vectors, test_source['target'])\n",
        "\n",
        "UNIQUE_WORDS_N = len(vocabulary)\n",
        "UNIQUE_LABELS_N = len(set(train_source['target']))\n",
        "model = nn.Linear(UNIQUE_WORDS_N, UNIQUE_LABELS_N)\n",
        "\n",
        "scheduler = lambda optim: torch.optim.lr_scheduler.ReduceLROnPlateau(optim, patience=5, factor=0.5, verbose=True)\n",
        "\n",
        "best_val_loss, best_model = train_eval_loop(model=model,\n",
        "                                            train_dataset=train_dataset,\n",
        "                                            val_dataset=test_dataset,\n",
        "                                            criterion=F.cross_entropy,\n",
        "                                            lr=LR,\n",
        "                                            epoch_n=EPOCH,\n",
        "                                            batch_size=BATCH,\n",
        "                                            l2_reg_alpha=L2,\n",
        "                                            lr_scheduler_ctor=scheduler)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DkfYkV3lFnf4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_pred = predict_with_model(best_model, train_dataset)\n",
        "train_loss = F.cross_entropy(torch.from_numpy(train_pred),torch.from_numpy(train_source['target']).long())\n",
        "test_pred = predict_with_model(best_model, test_dataset)\n",
        "test_loss = F.cross_entropy(torch.from_numpy(test_pred), torch.from_numpy(test_source['target']).long())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NqKYYFLpFnor",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print('Среднее значение функции потерь на обучении', float(train_loss))\n",
        "print('Доля верных ответов', accuracy_score(train_source['target'], train_pred.argmax(-1)))\n",
        "print()\n",
        "print('Среднее значение функции потерь на валидации', float(test_loss))\n",
        "print('Доля верных ответов', accuracy_score(test_source['target'], test_pred.argmax(-1)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fsj_AZ4RFnxa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "results2.append({'mode': 'tfidf', 'train_accuracy': accuracy_score(train_source['target'], train_pred.argmax(-1)), 'test_accuracy': accuracy_score(test_source['target'], test_pred.argmax(-1)), 'train_loss': float(train_loss), 'test_loss': float(test_loss)})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aozXcPQTHvtv",
        "colab_type": "text"
      },
      "source": [
        "#RESULTS: top n-gram"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GvDxYOZxFn6l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pd.DataFrame(results2).sort_values(by=['test_accuracy'], ascending=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pAaCqmbFFoBm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0DWIESHHFoLc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QtqpHOV9ED4w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HuD0tslJED99",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DVXSixrR7vU1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d4RW1Je5rDim",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import scipy.sparse\n",
        "import torch\n",
        "\n",
        "tokenized_texts = train_tokenized\n",
        "word2id = vocabulary\n",
        "word2freq = word_doc_freq\n",
        "\n",
        "result1 = scipy.sparse.dok_matrix((len(tokenized_texts), len(word2id)), dtype='float32')\n",
        "for text_i, text in enumerate(tokenized_texts):\n",
        "    for token in text:\n",
        "        if token in word2id:\n",
        "            result1[text_i, word2id[token]] += 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L_KZU3_vEa82",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "result = scipy.sparse.dok_matrix((len(set(train_source['target'])), len(word2id)), dtype='float32')\n",
        "for text_i, text in enumerate(tokenized_texts):\n",
        "    for token in text:\n",
        "        if token in word2id:\n",
        "            result[train_source['target'][text_i], word2id[token]] += 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZHVg2lxP73k9",
        "colab_type": "code",
        "outputId": "3d0df564-8fb2-49de-def3-a8d325d6324d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "result.shape"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(20, 21628)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "as8IpTJdvbW0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "result1_tf = result1.tocsr()\n",
        "result1_tf = result1_tf.multiply(1 / result1_tf.sum(1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q_X-lpLTvbUs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c0493d27-b39e-40ed-91a8-5a460d2c8979"
      },
      "source": [
        "result1_tf.sum(0).shape"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1, 21628)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vomoZxhREp-X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "_, counts_target = np.unique(train_source['target'], return_counts=True)\n",
        "# pwl = result / result.sum()\n",
        "pwl = result / counts_target.reshape(20, 1)\n",
        "pw = result.sum(0) / result.sum()\n",
        "pl = counts_target / counts_target.sum()\n",
        "# pmi = np.log2(pwl / pw / np.array([pl] * pwl.shape[1]).T)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2EshCjYCWdVg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "5441165f-6155-46ec-f768-259dc9e29711"
      },
      "source": [
        "pwl.shape, pw.shape, pl.shape"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((20, 21628), (1, 21628), (20,))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dyYYmRJZVR--",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pmi = scipy.sparse.dok_matrix((len(tokenized_texts), len(word2id)), dtype='float32')\n",
        "for text_i, text in enumerate(tokenized_texts):\n",
        "    for token in text:\n",
        "        if token in word2id:\n",
        "            pmi[text_i, word2id[token]] = np.log2(pwl[train_source['target'][text_i], word2id[token]] / (pw[0, word2id[token]] * pl[train_source['target'][text_i]]))\n",
        "# pmi = pmi.tocoo()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3S36MFXzv4uz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 163
        },
        "outputId": "14f1c919-e176-415c-84bf-ebc8060e02ff"
      },
      "source": [
        "result1_tf[0][0] / (result1_tf[0][0] * pl[0])"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-78-bd41e3e164c3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mresult1_tf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mresult1_tf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mpl\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m: 'coo_matrix' object does not support indexing"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EWUcl1cRaRMT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "outputId": "53039c3c-4696-4b60-bc2b-81c448d8c639"
      },
      "source": [
        "pwl[:,0] / (pw[0, 0] * pl.reshape(20, 1))"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "matrix([[6844.33551162],\n",
              "        [1472.90225213],\n",
              "        [1379.60255916],\n",
              "        [1517.69073931],\n",
              "        [1529.0496814 ],\n",
              "        [1897.13707402],\n",
              "        [ 423.14372555],\n",
              "        [2052.09164423],\n",
              "        [1647.71190308],\n",
              "        [1979.68065522],\n",
              "        [2057.02841064],\n",
              "        [3795.60846068],\n",
              "        [1666.96019066],\n",
              "        [3065.4003458 ],\n",
              "        [2368.58131697],\n",
              "        [6113.77143135],\n",
              "        [4671.59159706],\n",
              "        [6043.71757308],\n",
              "        [8648.6347432 ],\n",
              "        [8551.44512561]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RVPU7-HdbLsc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "23a3f5af-3423-4018-a990-b01cd8bb015a"
      },
      "source": [
        "pw[0, 0]"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.022657633"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oDZc9XrMk8jC",
        "colab_type": "code",
        "outputId": "6133a4ad-723e-4203-9bd7-0a6a51521fbf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "print(type(pmi))\n",
        "print(pmi.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'scipy.sparse.dok.dok_matrix'>\n",
            "(11314, 21628)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DQfA992AdtzX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "_pmi = pmi.tocsc()\n",
        "_pmi -= np.array([_pmi.min()] * _pmi.shape[1]).reshape(1, -1)\n",
        "# _pmi /= (_pmi.max() + 1e-6)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f4_uJsAujTs2",
        "colab_type": "code",
        "outputId": "242cf264-0c6d-4761-9120-0fd3e058f794",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "type(_pmi)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "numpy.matrix"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J1rM-68FJLcP",
        "colab_type": "code",
        "outputId": "262e250f-d074-42ea-b31a-148a5274a11b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "torch.from_numpy(pmi.toarray()).float().min()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(-7.9936)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 229
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YER7-ZDqGMA1",
        "colab_type": "code",
        "outputId": "265cf8d5-fc7b-4a4b-a3f4-e17d403ea9d7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        }
      },
      "source": [
        "pwl / np.array([pl] * pwl.shape[1]).T"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "matrix([[0.04295434, 0.01653973, 0.01579164, ..., 0.        , 0.        ,\n",
              "         0.        ],\n",
              "        [0.0112466 , 0.00941316, 0.0074232 , ..., 0.        , 0.        ,\n",
              "         0.        ],\n",
              "        [0.01066046, 0.00707015, 0.00871617, ..., 0.        , 0.        ,\n",
              "         0.        ],\n",
              "        ...,\n",
              "        [0.04456745, 0.01430789, 0.0144005 , ..., 0.        , 0.        ,\n",
              "         0.        ],\n",
              "        [0.05258175, 0.01990946, 0.01938996, ..., 0.        , 0.        ,\n",
              "         0.        ],\n",
              "        [0.04215173, 0.01560341, 0.01378504, ..., 0.        , 0.        ,\n",
              "         0.        ]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 200
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vFUdOj1fNIVw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2MRUYqrMkm02",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test = vectorize_texts(train_tokenized, vocabulary, word_doc_freq, mode='tfidf')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ma5rr9-EpydM",
        "colab_type": "code",
        "outputId": "7e333c13-1406-4277-fc7a-d848301fb4d1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "test.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(11314, 21628)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bgFupkl4psM1",
        "colab_type": "code",
        "outputId": "bd5e2126-b1b3-4361-9453-918cd90bd24d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "test = test.tocsc()\n",
        "print(type(test.min()))\n",
        "test -= test.min()\n",
        "test /= (test.max() + 1e-6)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'numpy.float32'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LClbYAjOFB33",
        "colab_type": "code",
        "outputId": "187fc20d-3c3b-4a79-b77c-9f38322d13fa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# torch.from_numpy(pwl.toarray()).max()\n",
        "pw.max()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.022657633"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 132
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J8baB1hjtxNI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pwc = (result > 0).astype('float32').power(-1).multiply(word2freq)\n",
        "pw = result.sum(0) / result.sum()\n",
        "pwc1 = result.sum(0) / result.shape[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5DDmb5wzDmbw",
        "colab_type": "code",
        "outputId": "e6dbd80b-87a5-4408-bfb7-f10b2fbcb295",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(set(train_source['target']))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "20"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 114
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bo63oGVQZvUw",
        "colab_type": "code",
        "outputId": "e929c05b-e674-4526-c00c-82a22a556f37",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "torch.from_numpy(pwc.toarray())[0].max()\n",
        "# pwc1.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(0.6015)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 126
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eFcOoU85vMza",
        "colab_type": "code",
        "outputId": "251d1bb1-5726-43a0-b795-4906271653a4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "word2freq.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(21628,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 105
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w-KjjRiO19_e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "result = result.tocsr()\n",
        "p_w_l = result / result.shape[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a5Svm2_Or5_1",
        "colab_type": "code",
        "outputId": "bc22bdd8-1e42-4aea-9e12-73f684199995",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "result = result.tocsr()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<11314x21628 sparse matrix of type '<class 'numpy.float32'>'\n",
              "\twith 1126792 stored elements in Compressed Sparse Row format>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3oWjgluBr6CV",
        "colab_type": "code",
        "outputId": "93654797-fc2c-419f-90e8-8beb3c12dd20",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "result.shape[0]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "11314"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FGaEgwGxr6He",
        "colab_type": "code",
        "outputId": "095e19c3-3342-4abb-80d3-6d3c0103d421",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "word2freq.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(21628,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_R2KrjofVtSe",
        "colab_type": "code",
        "outputId": "f218acc6-c8c1-4490-93a2-f3ab43180a34",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "1 / word2freq.max()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.412484426837751"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UdlfANA304_6",
        "colab_type": "code",
        "outputId": "083b6cc7-83b6-4952-99ea-289cef061f6c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "\n",
        "torch.from_numpy((result > 0).astype('float32').toarray()).float()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0., 1., 1.,  ..., 0., 0., 0.],\n",
              "        [0., 1., 1.,  ..., 0., 0., 0.],\n",
              "        [1., 1., 1.,  ..., 0., 0., 0.],\n",
              "        ...,\n",
              "        [1., 1., 1.,  ..., 0., 0., 0.],\n",
              "        [1., 1., 1.,  ..., 0., 0., 0.],\n",
              "        [0., 0., 0.,  ..., 0., 0., 0.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XMVLeMpDVTCL",
        "colab_type": "code",
        "outputId": "0af7e92d-f8d4-4f92-f3f5-7d2d3e317af6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "(result > 0).astype('float32').multiply(1 / word2freq).shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(11314, 21628)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZLQVFRQ-VdtG",
        "colab_type": "code",
        "outputId": "c26db061-9c85-4d1b-82c3-aea1bbde46ce",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "torch.from_numpy((result > 0).astype('float32').multiply(1 / word2freq).toarray()).float()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.0000, 1.6626, 1.6907,  ..., 0.0000, 0.0000, 0.0000],\n",
              "        [0.0000, 1.6626, 1.6907,  ..., 0.0000, 0.0000, 0.0000],\n",
              "        [1.4125, 1.6626, 1.6907,  ..., 0.0000, 0.0000, 0.0000],\n",
              "        ...,\n",
              "        [1.4125, 1.6626, 1.6907,  ..., 0.0000, 0.0000, 0.0000],\n",
              "        [1.4125, 1.6626, 1.6907,  ..., 0.0000, 0.0000, 0.0000],\n",
              "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TBHKp2Uc1JIi",
        "colab_type": "code",
        "outputId": "4fe51ef0-8f7c-42dc-94a4-844ec13b3f76",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "result.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(11314, 21628)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LO2atxDDMvuU",
        "colab_type": "code",
        "outputId": "d4f6523d-a904-4942-888c-57cce9efe5a0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "result.sum(0)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "matrix([[3.9264e+04, 2.0121e+04, 1.8074e+04, ..., 1.0000e+01, 1.0000e+01,\n",
              "         8.0000e+00]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-GuGnEzzWwkV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_s0S_efGYBHC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RoeRcPlrYBJ_",
        "colab_type": "code",
        "outputId": "fa7cff07-74b6-4bab-c452-a9ca5bb353c9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "torch.from_numpy(train_vectors.toarray()).float().shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([11314, 21628])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FS_QetMFYBZ3",
        "colab_type": "code",
        "outputId": "a6e88ecb-8e34-4d7e-a4f6-e1c0315cc687",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "train_source['target'].shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(11314,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JVVqhePUW3Ma",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}