{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "task1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "BWSRGhKL3cpq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!git clone https://github.com/Samsung-IT-Academy/stepik-dl-nlp.git && pip install -r stepik-dl-nlp/requirements.txt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I16MHqRD3jTd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sys; sys.path.append('./stepik-dl-nlp')\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import sklearn\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "import numpy as np\n",
        "import scipy.sparse\n",
        "import pandas as pd\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "import collections\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "import dlnlputils\n",
        "from dlnlputils.data import tokenize_text_simple_regex, tokenize_corpus, build_vocabulary, vectorize_texts, SparseFeaturesDataset\n",
        "from dlnlputils.pipeline import train_eval_loop, predict_with_model, init_random_seed\n",
        "\n",
        "init_random_seed()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cr-0l967LqhV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "results = []"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6tB97gSP4j8A",
        "colab_type": "text"
      },
      "source": [
        "#VECTORIZATION_MODE: tfidf"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X7gOsWaar_xt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "VECTORIZATION_MODE = 'tfidf'\n",
        "LR = 1e-1\n",
        "L2 = 0\n",
        "BATCH = 32\n",
        "EPOCH = 200"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jgl_7JDW3qR2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_source = fetch_20newsgroups(subset='train')\n",
        "test_source = fetch_20newsgroups(subset='test')\n",
        "\n",
        "train_tokenized = tokenize_corpus(train_source['data'])\n",
        "test_tokenized = tokenize_corpus(test_source['data'])\n",
        "\n",
        "MAX_DF = 0.8\n",
        "MIN_COUNT = 5\n",
        "vocabulary, word_doc_freq = build_vocabulary(train_tokenized, max_doc_freq=MAX_DF, min_count=MIN_COUNT)\n",
        "\n",
        "train_vectors = vectorize_texts(train_tokenized, vocabulary, word_doc_freq, mode=VECTORIZATION_MODE)\n",
        "test_vectors = vectorize_texts(test_tokenized, vocabulary, word_doc_freq, mode=VECTORIZATION_MODE)\n",
        "\n",
        "train_dataset = SparseFeaturesDataset(train_vectors, train_source['target'])\n",
        "test_dataset = SparseFeaturesDataset(test_vectors, test_source['target'])\n",
        "\n",
        "UNIQUE_WORDS_N = len(vocabulary)\n",
        "UNIQUE_LABELS_N = len(set(train_source['target']))\n",
        "model = nn.Linear(UNIQUE_WORDS_N, UNIQUE_LABELS_N)\n",
        "\n",
        "scheduler = lambda optim: torch.optim.lr_scheduler.ReduceLROnPlateau(optim, patience=5, factor=0.5, verbose=True)\n",
        "\n",
        "best_val_loss, best_model = train_eval_loop(model=model,\n",
        "                                            train_dataset=train_dataset,\n",
        "                                            val_dataset=test_dataset,\n",
        "                                            criterion=F.cross_entropy,\n",
        "                                            lr=LR,\n",
        "                                            epoch_n=EPOCH,\n",
        "                                            batch_size=BATCH,\n",
        "                                            l2_reg_alpha=L2,\n",
        "                                            lr_scheduler_ctor=scheduler)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4a_QSpSKKBT8",
        "colab_type": "code",
        "outputId": "4e73d38e-8917-4d33-a40f-3acadae8ea25",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "train_pred = predict_with_model(best_model, train_dataset)\n",
        "train_loss = F.cross_entropy(torch.from_numpy(train_pred),torch.from_numpy(train_source['target']).long())\n",
        "test_pred = predict_with_model(best_model, test_dataset)\n",
        "test_loss = F.cross_entropy(torch.from_numpy(test_pred), torch.from_numpy(test_source['target']).long())"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 354/353.5625 [00:01<00:00, 184.61it/s]\n",
            "236it [00:01, 180.98it/s]                             \n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-TTsqKJDDRet",
        "colab_type": "code",
        "outputId": "9f3ceadb-42cd-4bd7-8050-0f8267431868",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "print('Среднее значение функции потерь на обучении', float(train_loss))\n",
        "print('Доля верных ответов', accuracy_score(train_source['target'], train_pred.argmax(-1)))\n",
        "print()\n",
        "print('Среднее значение функции потерь на валидации', float(test_loss))\n",
        "print('Доля верных ответов', accuracy_score(test_source['target'], test_pred.argmax(-1)))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Среднее значение функции потерь на обучении 0.004174976609647274\n",
            "Доля верных ответов 0.9992929114371575\n",
            "\n",
            "Среднее значение функции потерь на валидации 0.9288815259933472\n",
            "Доля верных ответов 0.7712426978226234\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hTU5gG7pL-o5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "results.append({'mode': 'tfidf', 'train_accuracy': accuracy_score(train_source['target'], train_pred.argmax(-1)), 'test_accuracy': accuracy_score(test_source['target'], test_pred.argmax(-1)), 'train_loss': float(train_loss), 'test_loss': float(test_loss)})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t8bWXN2_KO5K",
        "colab_type": "text"
      },
      "source": [
        "#VECTORIZATION_MODE: tf"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HSy2jLudr9_m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "VECTORIZATION_MODE = 'tf'\n",
        "LR = 1e-1\n",
        "L2 = 0\n",
        "BATCH = 32\n",
        "EPOCH = 200"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PIGxp1FQDRhl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_source = fetch_20newsgroups(subset='train')\n",
        "test_source = fetch_20newsgroups(subset='test')\n",
        "\n",
        "train_tokenized = tokenize_corpus(train_source['data'])\n",
        "test_tokenized = tokenize_corpus(test_source['data'])\n",
        "\n",
        "MAX_DF = 0.8\n",
        "MIN_COUNT = 5\n",
        "vocabulary, word_doc_freq = build_vocabulary(train_tokenized, max_doc_freq=MAX_DF, min_count=MIN_COUNT)\n",
        "\n",
        "train_vectors = vectorize_texts(train_tokenized, vocabulary, word_doc_freq, mode=VECTORIZATION_MODE)\n",
        "test_vectors = vectorize_texts(test_tokenized, vocabulary, word_doc_freq, mode=VECTORIZATION_MODE)\n",
        "\n",
        "train_dataset = SparseFeaturesDataset(train_vectors, train_source['target'])\n",
        "test_dataset = SparseFeaturesDataset(test_vectors, test_source['target'])\n",
        "\n",
        "UNIQUE_WORDS_N = len(vocabulary)\n",
        "UNIQUE_LABELS_N = len(set(train_source['target']))\n",
        "model = nn.Linear(UNIQUE_WORDS_N, UNIQUE_LABELS_N)\n",
        "\n",
        "scheduler = lambda optim: torch.optim.lr_scheduler.ReduceLROnPlateau(optim, patience=5, factor=0.5, verbose=True)\n",
        "\n",
        "best_val_loss, best_model = train_eval_loop(model=model,\n",
        "                                            train_dataset=train_dataset,\n",
        "                                            val_dataset=test_dataset,\n",
        "                                            criterion=F.cross_entropy,\n",
        "                                            lr=LR,\n",
        "                                            epoch_n=EPOCH,\n",
        "                                            batch_size=BATCH,\n",
        "                                            l2_reg_alpha=L2,\n",
        "                                            lr_scheduler_ctor=scheduler)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X0jUvVl7KUtT",
        "colab_type": "code",
        "outputId": "2f5e952b-f95b-48c3-b6b8-0c49910e81e2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "train_pred = predict_with_model(best_model, train_dataset)\n",
        "train_loss = F.cross_entropy(torch.from_numpy(train_pred),torch.from_numpy(train_source['target']).long())\n",
        "test_pred = predict_with_model(best_model, test_dataset)\n",
        "test_loss = F.cross_entropy(torch.from_numpy(test_pred), torch.from_numpy(test_source['target']).long())"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 354/353.5625 [00:01<00:00, 180.62it/s]\n",
            "236it [00:01, 182.84it/s]                             \n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QPwR6qXADRke",
        "colab_type": "code",
        "outputId": "268c842d-557e-4714-c643-bc0387433b44",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "print('Среднее значение функции потерь на обучении', float(train_loss))\n",
        "print('Доля верных ответов', accuracy_score(train_source['target'], train_pred.argmax(-1)))\n",
        "print()\n",
        "print('Среднее значение функции потерь на валидации', float(test_loss))\n",
        "print('Доля верных ответов', accuracy_score(test_source['target'], test_pred.argmax(-1)))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Среднее значение функции потерь на обучении 0.005827018991112709\n",
            "Доля верных ответов 0.9992045253668022\n",
            "\n",
            "Среднее значение функции потерь на валидации 0.6565795540809631\n",
            "Доля верных ответов 0.8145246946362188\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xx50ILe-Mdld",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "results.append({'mode': 'tf', 'train_accuracy': accuracy_score(train_source['target'], train_pred.argmax(-1)), 'test_accuracy': accuracy_score(test_source['target'], test_pred.argmax(-1)), 'train_loss': float(train_loss), 'test_loss': float(test_loss)})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pfj2qq4WStz2",
        "colab_type": "text"
      },
      "source": [
        "#VECTORIZATION_MODE: idf"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u85FFan_r73t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "VECTORIZATION_MODE = 'idf'\n",
        "LR = 1e-1\n",
        "L2 = 0\n",
        "BATCH = 32\n",
        "EPOCH = 200"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LFVkj7zCDRm8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_source = fetch_20newsgroups(subset='train')\n",
        "test_source = fetch_20newsgroups(subset='test')\n",
        "\n",
        "train_tokenized = tokenize_corpus(train_source['data'])\n",
        "test_tokenized = tokenize_corpus(test_source['data'])\n",
        "\n",
        "MAX_DF = 0.8\n",
        "MIN_COUNT = 5\n",
        "vocabulary, word_doc_freq = build_vocabulary(train_tokenized, max_doc_freq=MAX_DF, min_count=MIN_COUNT)\n",
        "\n",
        "train_vectors = vectorize_texts(train_tokenized, vocabulary, word_doc_freq, mode=VECTORIZATION_MODE)\n",
        "test_vectors = vectorize_texts(test_tokenized, vocabulary, word_doc_freq, mode=VECTORIZATION_MODE)\n",
        "\n",
        "train_dataset = SparseFeaturesDataset(train_vectors, train_source['target'])\n",
        "test_dataset = SparseFeaturesDataset(test_vectors, test_source['target'])\n",
        "\n",
        "UNIQUE_WORDS_N = len(vocabulary)\n",
        "UNIQUE_LABELS_N = len(set(train_source['target']))\n",
        "model = nn.Linear(UNIQUE_WORDS_N, UNIQUE_LABELS_N)\n",
        "\n",
        "scheduler = lambda optim: torch.optim.lr_scheduler.ReduceLROnPlateau(optim, patience=5, factor=0.5, verbose=True)\n",
        "\n",
        "best_val_loss, best_model = train_eval_loop(model=model,\n",
        "                                            train_dataset=train_dataset,\n",
        "                                            val_dataset=test_dataset,\n",
        "                                            criterion=F.cross_entropy,\n",
        "                                            lr=LR,\n",
        "                                            epoch_n=EPOCH,\n",
        "                                            batch_size=BATCH,\n",
        "                                            l2_reg_alpha=L2,\n",
        "                                            lr_scheduler_ctor=scheduler)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TNnrAHdb3qUU",
        "colab_type": "code",
        "outputId": "2c0af359-aaad-43ab-d15b-ddcc32553c8c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "train_pred = predict_with_model(best_model, train_dataset)\n",
        "train_loss = F.cross_entropy(torch.from_numpy(train_pred),torch.from_numpy(train_source['target']).long())\n",
        "test_pred = predict_with_model(best_model, test_dataset)\n",
        "test_loss = F.cross_entropy(torch.from_numpy(test_pred), torch.from_numpy(test_source['target']).long())"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 354/353.5625 [00:01<00:00, 189.35it/s]\n",
            "236it [00:01, 188.52it/s]                             \n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VSuqTI4L3qWw",
        "colab_type": "code",
        "outputId": "ff950f2e-722d-48f3-be6a-5e97eb2aed37",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "print('Среднее значение функции потерь на обучении', float(train_loss))\n",
        "print('Доля верных ответов', accuracy_score(train_source['target'], train_pred.argmax(-1)))\n",
        "print()\n",
        "print('Среднее значение функции потерь на валидации', float(test_loss))\n",
        "print('Доля верных ответов', accuracy_score(test_source['target'], test_pred.argmax(-1)))"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Среднее значение функции потерь на обучении 0.021615294739603996\n",
            "Доля верных ответов 0.9984090507336044\n",
            "\n",
            "Среднее значение функции потерь на валидации 1.0027966499328613\n",
            "Доля верных ответов 0.7525225703664365\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0FB3eiL0MhiM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "results.append({'mode': 'idf', 'train_accuracy': accuracy_score(train_source['target'], train_pred.argmax(-1)), 'test_accuracy': accuracy_score(test_source['target'], test_pred.argmax(-1)), 'train_loss': float(train_loss), 'test_loss': float(test_loss)})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2uU5IAO6rb_d",
        "colab_type": "text"
      },
      "source": [
        "#VECTORIZATION_MODE: binary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lsgIPozqr1WQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "VECTORIZATION_MODE = 'bin'\n",
        "LR = 1e-1\n",
        "L2 = 0\n",
        "BATCH = 32\n",
        "EPOCH = 200"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7DU-Vo9SpDlq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_source = fetch_20newsgroups(subset='train')\n",
        "test_source = fetch_20newsgroups(subset='test')\n",
        "\n",
        "train_tokenized = tokenize_corpus(train_source['data'])\n",
        "test_tokenized = tokenize_corpus(test_source['data'])\n",
        "\n",
        "MAX_DF = 0.8\n",
        "MIN_COUNT = 5\n",
        "vocabulary, word_doc_freq = build_vocabulary(train_tokenized, max_doc_freq=MAX_DF, min_count=MIN_COUNT)\n",
        "\n",
        "train_vectors = vectorize_texts(train_tokenized, vocabulary, word_doc_freq, mode=VECTORIZATION_MODE)\n",
        "test_vectors = vectorize_texts(test_tokenized, vocabulary, word_doc_freq, mode=VECTORIZATION_MODE)\n",
        "\n",
        "train_dataset = SparseFeaturesDataset(train_vectors, train_source['target'])\n",
        "test_dataset = SparseFeaturesDataset(test_vectors, test_source['target'])\n",
        "\n",
        "UNIQUE_WORDS_N = len(vocabulary)\n",
        "UNIQUE_LABELS_N = len(set(train_source['target']))\n",
        "model = nn.Linear(UNIQUE_WORDS_N, UNIQUE_LABELS_N)\n",
        "\n",
        "scheduler = lambda optim: torch.optim.lr_scheduler.ReduceLROnPlateau(optim, patience=5, factor=0.5, verbose=True)\n",
        "\n",
        "best_val_loss, best_model = train_eval_loop(model=model,\n",
        "                                            train_dataset=train_dataset,\n",
        "                                            val_dataset=test_dataset,\n",
        "                                            criterion=F.cross_entropy,\n",
        "                                            lr=LR,\n",
        "                                            epoch_n=EPOCH,\n",
        "                                            batch_size=BATCH,\n",
        "                                            l2_reg_alpha=L2,\n",
        "                                            lr_scheduler_ctor=scheduler)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jCrnU8WKr4NV",
        "colab_type": "code",
        "outputId": "0c280a7f-40f1-4fde-b293-84d53c818a73",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "train_pred = predict_with_model(best_model, train_dataset)\n",
        "train_loss = F.cross_entropy(torch.from_numpy(train_pred),torch.from_numpy(train_source['target']).long())\n",
        "test_pred = predict_with_model(best_model, test_dataset)\n",
        "test_loss = F.cross_entropy(torch.from_numpy(test_pred), torch.from_numpy(test_source['target']).long())"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 354/353.5625 [00:01<00:00, 193.50it/s]\n",
            "236it [00:01, 195.72it/s]                             \n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ivvtwx9ZrDgF",
        "colab_type": "code",
        "outputId": "c7ff8e13-5268-461d-b218-4038bac8fe6b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "print('Среднее значение функции потерь на обучении', float(train_loss))\n",
        "print('Доля верных ответов', accuracy_score(train_source['target'], train_pred.argmax(-1)))\n",
        "print()\n",
        "print('Среднее значение функции потерь на валидации', float(test_loss))\n",
        "print('Доля верных ответов', accuracy_score(test_source['target'], test_pred.argmax(-1)))"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Среднее значение функции потерь на обучении 0.0634421557188034\n",
            "Доля верных ответов 0.9934594307937069\n",
            "\n",
            "Среднее значение функции потерь на валидации 3.8096024990081787\n",
            "Доля верных ответов 0.7308815719596389\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MtbnKCZJMki_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "results.append({'mode': 'bin', 'train_accuracy': accuracy_score(train_source['target'], train_pred.argmax(-1)), 'test_accuracy': accuracy_score(test_source['target'], test_pred.argmax(-1)), 'train_loss': float(train_loss), 'test_loss': float(test_loss)})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fFuJoG5MsiMA",
        "colab_type": "text"
      },
      "source": [
        "#VECTORIZATION_MODE: pmi"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nSJXZXLiQsRB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def vectorize_texts_pmi(tokenized_texts, word2id, target):\n",
        "    word_count_per_label = scipy.sparse.dok_matrix((len(set(target)), len(word2id)), dtype='float32')\n",
        "    for text_i, text in enumerate(tokenized_texts):\n",
        "        for token in text:\n",
        "            if token in word2id:\n",
        "                word_count_per_label[target[text_i], word2id[token]] += 1\n",
        "\n",
        "    _, counts_target = np.unique(target, return_counts=True)\n",
        "    pwl = word_count_per_label / word_count_per_label.sum()\n",
        "    pw = word_count_per_label.sum(0) / word_count_per_label.sum()\n",
        "    pl = counts_target / counts_target.sum()\n",
        "    \n",
        "    pmi = scipy.sparse.dok_matrix((len(tokenized_texts), len(word2id)), dtype='float32')\n",
        "    for text_i, text in enumerate(tokenized_texts):\n",
        "        for token in text:\n",
        "            if token in word2id:\n",
        "                pmi[text_i, word2id[token]] = np.log2(pwl[target[text_i], word2id[token]] / (pw[0, word2id[token]] * pl[target[text_i]]))\n",
        "      \n",
        "    return scipy.sparse.csr_matrix(pmi)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "liEe9XdO9O_s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_source = fetch_20newsgroups(subset='train')\n",
        "test_source = fetch_20newsgroups(subset='test')\n",
        "\n",
        "train_tokenized = tokenize_corpus(train_source['data'])\n",
        "test_tokenized = tokenize_corpus(test_source['data'])\n",
        "\n",
        "MAX_DF = 0.8\n",
        "MIN_COUNT = 5\n",
        "vocabulary, word_doc_freq = build_vocabulary(train_tokenized, max_doc_freq=MAX_DF, min_count=MIN_COUNT)\n",
        "\n",
        "train_vectors = vectorize_texts_pmi(train_tokenized, vocabulary, train_source['target'])\n",
        "test_vectors = vectorize_texts_pmi(test_tokenized, vocabulary, test_source['target'])\n",
        "\n",
        "train_dataset = SparseFeaturesDataset(train_vectors, train_source['target'])\n",
        "test_dataset = SparseFeaturesDataset(test_vectors, test_source['target'])\n",
        "\n",
        "UNIQUE_WORDS_N = len(vocabulary)\n",
        "UNIQUE_LABELS_N = len(set(train_source['target']))\n",
        "model = nn.Linear(UNIQUE_WORDS_N, UNIQUE_LABELS_N)\n",
        "\n",
        "scheduler = lambda optim: torch.optim.lr_scheduler.ReduceLROnPlateau(optim, patience=5, factor=0.5, verbose=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LKXFMB_b-iNz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "LR = 1e-1\n",
        "L2 = 1e-3\n",
        "BATCH = 32\n",
        "EPOCH = 200"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rkR7Py9Se0bm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "best_val_loss, best_model = train_eval_loop(model=model,\n",
        "                                            train_dataset=train_dataset,\n",
        "                                            val_dataset=test_dataset,\n",
        "                                            criterion=F.cross_entropy,\n",
        "                                            lr=LR,\n",
        "                                            epoch_n=EPOCH,\n",
        "                                            batch_size=BATCH,\n",
        "                                            l2_reg_alpha=L2,\n",
        "                                            lr_scheduler_ctor=scheduler)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "otRHujSAe0Yh",
        "colab_type": "code",
        "outputId": "1e20ad90-cd5e-46c8-eb09-5ea69c0d4dea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "train_pred = predict_with_model(best_model, train_dataset)\n",
        "train_loss = F.cross_entropy(torch.from_numpy(train_pred),torch.from_numpy(train_source['target']).long())\n",
        "test_pred = predict_with_model(best_model, test_dataset)\n",
        "test_loss = F.cross_entropy(torch.from_numpy(test_pred), torch.from_numpy(test_source['target']).long())"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 354/353.5625 [00:01<00:00, 178.81it/s]\n",
            "236it [00:01, 177.61it/s]                             \n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "miAk1GiXe0WR",
        "colab_type": "code",
        "outputId": "a452c78f-7ed7-4648-8c70-2e4b1bc5993d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "print('Среднее значение функции потерь на обучении', float(train_loss))\n",
        "print('Доля верных ответов', accuracy_score(train_source['target'], train_pred.argmax(-1)))\n",
        "print()\n",
        "print('Среднее значение функции потерь на валидации', float(test_loss))\n",
        "print('Доля верных ответов', accuracy_score(test_source['target'], test_pred.argmax(-1)))"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Среднее значение функции потерь на обучении 0.05344357341527939\n",
            "Доля верных ответов 0.9992045253668022\n",
            "\n",
            "Среднее значение функции потерь на валидации 0.317936509847641\n",
            "Доля верных ответов 0.9358736059479554\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yEePSPi2CmLj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "results.append({'mode': 'pmi', 'train_accuracy': accuracy_score(train_source['target'], train_pred.argmax(-1)), 'test_accuracy': accuracy_score(test_source['target'], test_pred.argmax(-1)), 'train_loss': float(train_loss), 'test_loss': float(test_loss)})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m5b2MtZA37KJ",
        "colab_type": "text"
      },
      "source": [
        "#VECTORIZATION_MODE: lsa"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HF56XQsE2vy2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def vectorize_texts_lsa(tokenized_texts_train, tokenized_texts_test, word2id, n_components=100, scale=True):\n",
        "\n",
        "    train_word_count = scipy.sparse.dok_matrix((len(tokenized_texts_train), len(word2id)), dtype='float32')\n",
        "    for text_i, text in enumerate(tokenized_texts_train):\n",
        "        for token in text:\n",
        "            if token in word2id:\n",
        "                train_word_count[text_i, word2id[token]] += 1\n",
        "\n",
        "    test_word_count = scipy.sparse.dok_matrix((len(tokenized_texts_test), len(word2id)), dtype='float32')\n",
        "    for text_i, text in enumerate(tokenized_texts_test):\n",
        "        for token in text:\n",
        "            if token in word2id:\n",
        "                test_word_count[text_i, word2id[token]] += 1\n",
        "\n",
        "    lsa = sklearn.decomposition.TruncatedSVD(n_components)\n",
        "    train_vectors = lsa.fit_transform(train_word_count)\n",
        "    test_vectors = lsa.transform(test_word_count)\n",
        "\n",
        "    if scale:\n",
        "        norm = sklearn.preprocessing.Normalizer(copy=False)\n",
        "        train_vectors = norm.fit_transform(train_vectors)\n",
        "        test_vectors = norm.transform(test_vectors)\n",
        "\n",
        "    return scipy.sparse.csr_matrix(train_vectors), scipy.sparse.csr_matrix(test_vectors)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DN5ci3qItNbr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_source = fetch_20newsgroups(subset='train')\n",
        "test_source = fetch_20newsgroups(subset='test')\n",
        "\n",
        "train_tokenized = tokenize_corpus(train_source['data'])\n",
        "test_tokenized = tokenize_corpus(test_source['data'])\n",
        "\n",
        "MAX_DF = 0.8\n",
        "MIN_COUNT = 5\n",
        "vocabulary, word_doc_freq = build_vocabulary(train_tokenized, max_doc_freq=MAX_DF, min_count=MIN_COUNT)\n",
        "\n",
        "train_vectors, test_vectors = vectorize_texts_lsa(train_tokenized, test_tokenized, vocabulary)\n",
        "\n",
        "train_dataset = SparseFeaturesDataset(train_vectors, train_source['target'])\n",
        "test_dataset = SparseFeaturesDataset(test_vectors, test_source['target'])\n",
        "\n",
        "UNIQUE_WORDS_N = train_vectors.shape[1]\n",
        "UNIQUE_LABELS_N = len(set(train_source['target']))\n",
        "model = nn.Linear(UNIQUE_WORDS_N, UNIQUE_LABELS_N)\n",
        "\n",
        "scheduler = lambda optim: torch.optim.lr_scheduler.ReduceLROnPlateau(optim, patience=5, factor=0.5, verbose=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vi2VpRyhxEmu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "LR = 1e-1\n",
        "L2 = 0\n",
        "BATCH = 32\n",
        "EPOCH = 200"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r61Z5H1_xEsd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "best_val_loss, best_model = train_eval_loop(model=model,\n",
        "                                            train_dataset=train_dataset,\n",
        "                                            val_dataset=test_dataset,\n",
        "                                            criterion=F.cross_entropy,\n",
        "                                            lr=LR,\n",
        "                                            epoch_n=EPOCH,\n",
        "                                            batch_size=BATCH,\n",
        "                                            l2_reg_alpha=L2,\n",
        "                                            lr_scheduler_ctor=scheduler)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eMC5FcYrxEqB",
        "colab_type": "code",
        "outputId": "eb6e2afa-6bd6-4901-bc5c-95ab60f0d0c6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "train_pred = predict_with_model(best_model, train_dataset)\n",
        "train_loss = F.cross_entropy(torch.from_numpy(train_pred),torch.from_numpy(train_source['target']).long())\n",
        "test_pred = predict_with_model(best_model, test_dataset)\n",
        "test_loss = F.cross_entropy(torch.from_numpy(test_pred), torch.from_numpy(test_source['target']).long())"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 354/353.5625 [00:01<00:00, 271.31it/s]\n",
            "236it [00:00, 281.45it/s]                             \n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2CBERrRy09rv",
        "colab_type": "code",
        "outputId": "10355160-43cf-4f11-84bd-43f2aa2814c4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "print('Среднее значение функции потерь на обучении', float(train_loss))\n",
        "print('Доля верных ответов', accuracy_score(train_source['target'], train_pred.argmax(-1)))\n",
        "print()\n",
        "print('Среднее значение функции потерь на валидации', float(test_loss))\n",
        "print('Доля верных ответов', accuracy_score(test_source['target'], test_pred.argmax(-1)))"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Среднее значение функции потерь на обучении 0.9047325849533081\n",
            "Доля верных ответов 0.72379353013965\n",
            "\n",
            "Среднее значение функции потерь на валидации 1.313632845878601\n",
            "Доля верных ответов 0.6072756240042485\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mr1oapS009zy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "results.append({'mode': 'lsa', 'train_accuracy': accuracy_score(train_source['target'], train_pred.argmax(-1)), 'test_accuracy': accuracy_score(test_source['target'], test_pred.argmax(-1)), 'train_loss': float(train_loss), 'test_loss': float(test_loss)})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gmUYJFWk4cF5",
        "colab_type": "text"
      },
      "source": [
        "# RESULTS: top vectorization mode"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GDTRiP3_zC8C",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "outputId": "ed05e148-2bcb-4e1c-9746-974460cd59c2"
      },
      "source": [
        "pd.DataFrame(results).sort_values(by=['test_accuracy'], ascending=False)"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>mode</th>\n",
              "      <th>train_accuracy</th>\n",
              "      <th>test_accuracy</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>test_loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>pmi</td>\n",
              "      <td>0.999205</td>\n",
              "      <td>0.935874</td>\n",
              "      <td>0.053444</td>\n",
              "      <td>0.317937</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>tf</td>\n",
              "      <td>0.999205</td>\n",
              "      <td>0.814525</td>\n",
              "      <td>0.005827</td>\n",
              "      <td>0.656580</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>tfidf</td>\n",
              "      <td>0.999293</td>\n",
              "      <td>0.771243</td>\n",
              "      <td>0.004175</td>\n",
              "      <td>0.928882</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>idf</td>\n",
              "      <td>0.998409</td>\n",
              "      <td>0.752523</td>\n",
              "      <td>0.021615</td>\n",
              "      <td>1.002797</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>bin</td>\n",
              "      <td>0.993459</td>\n",
              "      <td>0.730882</td>\n",
              "      <td>0.063442</td>\n",
              "      <td>3.809602</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>lsa</td>\n",
              "      <td>0.723794</td>\n",
              "      <td>0.607276</td>\n",
              "      <td>0.904733</td>\n",
              "      <td>1.313633</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    mode  train_accuracy  test_accuracy  train_loss  test_loss\n",
              "5    pmi        0.999205       0.935874    0.053444   0.317937\n",
              "1     tf        0.999205       0.814525    0.005827   0.656580\n",
              "0  tfidf        0.999293       0.771243    0.004175   0.928882\n",
              "2    idf        0.998409       0.752523    0.021615   1.002797\n",
              "3    bin        0.993459       0.730882    0.063442   3.809602\n",
              "4    lsa        0.723794       0.607276    0.904733   1.313633"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4heMM-BU8auz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rVZ2vXRi9XIc",
        "colab_type": "text"
      },
      "source": [
        "#STEMMING"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MAJpPZzi9Wbs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import nltk\n",
        "# nltk.download('wordnet')\n",
        "sno = nltk.stem.SnowballStemmer('english')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JfHYjvz19ccU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "results1 = []"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uKcvnEmi8u7A",
        "colab_type": "text"
      },
      "source": [
        "#STEMMING: VECTORIZATION_MODE: pmi"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gsKs4U11tNN6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_source = fetch_20newsgroups(subset='train')\n",
        "test_source = fetch_20newsgroups(subset='test')\n",
        "\n",
        "train_tokenized = tokenize_corpus(train_source['data'])\n",
        "test_tokenized = tokenize_corpus(test_source['data'])\n",
        "\n",
        "train_tokenized = [[sno.stem(word) for word in doc] for doc in train_tokenized]\n",
        "test_tokenized = [[sno.stem(word) for word in doc] for doc in test_tokenized]\n",
        "\n",
        "MAX_DF = 0.8\n",
        "MIN_COUNT = 5\n",
        "vocabulary, word_doc_freq = build_vocabulary(train_tokenized, max_doc_freq=MAX_DF, min_count=MIN_COUNT)\n",
        "\n",
        "train_vectors = vectorize_texts_pmi(train_tokenized, vocabulary, train_source['target'])\n",
        "test_vectors = vectorize_texts_pmi(test_tokenized, vocabulary, test_source['target'])\n",
        "\n",
        "train_dataset = SparseFeaturesDataset(train_vectors, train_source['target'])\n",
        "test_dataset = SparseFeaturesDataset(test_vectors, test_source['target'])\n",
        "\n",
        "UNIQUE_WORDS_N = len(vocabulary)\n",
        "UNIQUE_LABELS_N = len(set(train_source['target']))\n",
        "model = nn.Linear(UNIQUE_WORDS_N, UNIQUE_LABELS_N)\n",
        "\n",
        "scheduler = lambda optim: torch.optim.lr_scheduler.ReduceLROnPlateau(optim, patience=5, factor=0.5, verbose=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nmgv85NZ7vXR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "LR = 1e-1\n",
        "L2 = 1e-3\n",
        "BATCH = 32\n",
        "EPOCH = 200"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oE6lrmrk7ven",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "best_val_loss, best_model = train_eval_loop(model=model,\n",
        "                                            train_dataset=train_dataset,\n",
        "                                            val_dataset=test_dataset,\n",
        "                                            criterion=F.cross_entropy,\n",
        "                                            lr=LR,\n",
        "                                            epoch_n=EPOCH,\n",
        "                                            batch_size=BATCH,\n",
        "                                            l2_reg_alpha=L2,\n",
        "                                            lr_scheduler_ctor=scheduler)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "igVyNY227vck",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "db6669fb-092e-43fb-b5ea-464ded697fa3"
      },
      "source": [
        "train_pred = predict_with_model(best_model, train_dataset)\n",
        "train_loss = F.cross_entropy(torch.from_numpy(train_pred),torch.from_numpy(train_source['target']).long())\n",
        "test_pred = predict_with_model(best_model, test_dataset)\n",
        "test_loss = F.cross_entropy(torch.from_numpy(test_pred), torch.from_numpy(test_source['target']).long())"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 354/353.5625 [00:01<00:00, 202.29it/s]\n",
            "236it [00:01, 206.04it/s]                             \n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Hk-ZAC98JC3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "792e5f4b-dd9d-4075-a709-b751b685c585"
      },
      "source": [
        "print('Среднее значение функции потерь на обучении', float(train_loss))\n",
        "print('Доля верных ответов', accuracy_score(train_source['target'], train_pred.argmax(-1)))\n",
        "print()\n",
        "print('Среднее значение функции потерь на валидации', float(test_loss))\n",
        "print('Доля верных ответов', accuracy_score(test_source['target'], test_pred.argmax(-1)))"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Среднее значение функции потерь на обучении 0.05469982698559761\n",
            "Доля верных ответов 0.9993812975075128\n",
            "\n",
            "Среднее значение функции потерь на валидации 0.3036949932575226\n",
            "Доля верных ответов 0.942777482740308\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FayJkm-48JKk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "results1.append({'mode': 'pmi', 'train_accuracy': accuracy_score(train_source['target'], train_pred.argmax(-1)), 'test_accuracy': accuracy_score(test_source['target'], test_pred.argmax(-1)), 'train_loss': float(train_loss), 'test_loss': float(test_loss)})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WsrdnLrfDQ8G",
        "colab_type": "text"
      },
      "source": [
        "#STEMMING: VECTORIZATION_MODE: tf"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RE8PP7cj8JIH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "VECTORIZATION_MODE = 'tf'\n",
        "LR = 1e-1\n",
        "L2 = 0\n",
        "BATCH = 32\n",
        "EPOCH = 200"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FyGeCLuc8JGG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_source = fetch_20newsgroups(subset='train')\n",
        "test_source = fetch_20newsgroups(subset='test')\n",
        "\n",
        "train_tokenized = tokenize_corpus(train_source['data'])\n",
        "test_tokenized = tokenize_corpus(test_source['data'])\n",
        "\n",
        "train_tokenized = [[sno.stem(word) for word in doc] for doc in train_tokenized]\n",
        "test_tokenized = [[sno.stem(word) for word in doc] for doc in test_tokenized]\n",
        "\n",
        "MAX_DF = 0.8\n",
        "MIN_COUNT = 5\n",
        "vocabulary, word_doc_freq = build_vocabulary(train_tokenized, max_doc_freq=MAX_DF, min_count=MIN_COUNT)\n",
        "\n",
        "train_vectors = vectorize_texts(train_tokenized, vocabulary, word_doc_freq, mode=VECTORIZATION_MODE)\n",
        "test_vectors = vectorize_texts(test_tokenized, vocabulary, word_doc_freq, mode=VECTORIZATION_MODE)\n",
        "\n",
        "train_dataset = SparseFeaturesDataset(train_vectors, train_source['target'])\n",
        "test_dataset = SparseFeaturesDataset(test_vectors, test_source['target'])\n",
        "\n",
        "UNIQUE_WORDS_N = len(vocabulary)\n",
        "UNIQUE_LABELS_N = len(set(train_source['target']))\n",
        "model = nn.Linear(UNIQUE_WORDS_N, UNIQUE_LABELS_N)\n",
        "\n",
        "scheduler = lambda optim: torch.optim.lr_scheduler.ReduceLROnPlateau(optim, patience=5, factor=0.5, verbose=True)\n",
        "\n",
        "best_val_loss, best_model = train_eval_loop(model=model,\n",
        "                                            train_dataset=train_dataset,\n",
        "                                            val_dataset=test_dataset,\n",
        "                                            criterion=F.cross_entropy,\n",
        "                                            lr=LR,\n",
        "                                            epoch_n=EPOCH,\n",
        "                                            batch_size=BATCH,\n",
        "                                            l2_reg_alpha=L2,\n",
        "                                            lr_scheduler_ctor=scheduler)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hw75IcK67vae",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "859b08fe-7f45-44fc-e320-b28542b8fca0"
      },
      "source": [
        "train_pred = predict_with_model(best_model, train_dataset)\n",
        "train_loss = F.cross_entropy(torch.from_numpy(train_pred),torch.from_numpy(train_source['target']).long())\n",
        "test_pred = predict_with_model(best_model, test_dataset)\n",
        "test_loss = F.cross_entropy(torch.from_numpy(test_pred), torch.from_numpy(test_source['target']).long())"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 354/353.5625 [00:01<00:00, 189.97it/s]\n",
            "236it [00:01, 186.77it/s]                             \n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j4QE_JnkDZsE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "114d7ccc-e3ac-4656-9d79-a21aba3033e3"
      },
      "source": [
        "print('Среднее значение функции потерь на обучении', float(train_loss))\n",
        "print('Доля верных ответов', accuracy_score(train_source['target'], train_pred.argmax(-1)))\n",
        "print()\n",
        "print('Среднее значение функции потерь на валидации', float(test_loss))\n",
        "print('Доля верных ответов', accuracy_score(test_source['target'], test_pred.argmax(-1)))"
      ],
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Среднее значение функции потерь на обучении 0.010306037962436676\n",
            "Доля верных ответов 0.9992045253668022\n",
            "\n",
            "Среднее значение функции потерь на валидации 0.6652761101722717\n",
            "Доля верных ответов 0.8113382899628253\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kCrfz7RkDZ5e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "results1.append({'mode': 'tf', 'train_accuracy': accuracy_score(train_source['target'], train_pred.argmax(-1)), 'test_accuracy': accuracy_score(test_source['target'], test_pred.argmax(-1)), 'train_loss': float(train_loss), 'test_loss': float(test_loss)})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LfXPTmecDx0z",
        "colab_type": "text"
      },
      "source": [
        "#STEMMING: VECTORIZATION_MODE: tfidf"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2uY-9DIHDZ2_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "VECTORIZATION_MODE = 'tfidf'\n",
        "LR = 1e-1\n",
        "L2 = 0\n",
        "BATCH = 32\n",
        "EPOCH = 200"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ISPPPlT8D1LE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_source = fetch_20newsgroups(subset='train')\n",
        "test_source = fetch_20newsgroups(subset='test')\n",
        "\n",
        "train_tokenized = tokenize_corpus(train_source['data'])\n",
        "test_tokenized = tokenize_corpus(test_source['data'])\n",
        "\n",
        "train_tokenized = [[sno.stem(word) for word in doc] for doc in train_tokenized]\n",
        "test_tokenized = [[sno.stem(word) for word in doc] for doc in test_tokenized]\n",
        "\n",
        "MAX_DF = 0.8\n",
        "MIN_COUNT = 5\n",
        "vocabulary, word_doc_freq = build_vocabulary(train_tokenized, max_doc_freq=MAX_DF, min_count=MIN_COUNT)\n",
        "\n",
        "train_vectors = vectorize_texts(train_tokenized, vocabulary, word_doc_freq, mode=VECTORIZATION_MODE)\n",
        "test_vectors = vectorize_texts(test_tokenized, vocabulary, word_doc_freq, mode=VECTORIZATION_MODE)\n",
        "\n",
        "train_dataset = SparseFeaturesDataset(train_vectors, train_source['target'])\n",
        "test_dataset = SparseFeaturesDataset(test_vectors, test_source['target'])\n",
        "\n",
        "UNIQUE_WORDS_N = len(vocabulary)\n",
        "UNIQUE_LABELS_N = len(set(train_source['target']))\n",
        "model = nn.Linear(UNIQUE_WORDS_N, UNIQUE_LABELS_N)\n",
        "\n",
        "scheduler = lambda optim: torch.optim.lr_scheduler.ReduceLROnPlateau(optim, patience=5, factor=0.5, verbose=True)\n",
        "\n",
        "best_val_loss, best_model = train_eval_loop(model=model,\n",
        "                                            train_dataset=train_dataset,\n",
        "                                            val_dataset=test_dataset,\n",
        "                                            criterion=F.cross_entropy,\n",
        "                                            lr=LR,\n",
        "                                            epoch_n=EPOCH,\n",
        "                                            batch_size=BATCH,\n",
        "                                            l2_reg_alpha=L2,\n",
        "                                            lr_scheduler_ctor=scheduler)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LFPq1-NADZ0g",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "996a21cb-71fe-4251-b94c-d7ea7192d3dc"
      },
      "source": [
        "train_pred = predict_with_model(best_model, train_dataset)\n",
        "train_loss = F.cross_entropy(torch.from_numpy(train_pred),torch.from_numpy(train_source['target']).long())\n",
        "test_pred = predict_with_model(best_model, test_dataset)\n",
        "test_loss = F.cross_entropy(torch.from_numpy(test_pred), torch.from_numpy(test_source['target']).long())"
      ],
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 354/353.5625 [00:01<00:00, 203.15it/s]\n",
            "236it [00:01, 208.02it/s]                             \n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mRmlAokODZyX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "fd8899c7-9726-49c0-c5f5-fbedbfb6e001"
      },
      "source": [
        "print('Среднее значение функции потерь на обучении', float(train_loss))\n",
        "print('Доля верных ответов', accuracy_score(train_source['target'], train_pred.argmax(-1)))\n",
        "print()\n",
        "print('Среднее значение функции потерь на валидации', float(test_loss))\n",
        "print('Доля верных ответов', accuracy_score(test_source['target'], test_pred.argmax(-1)))"
      ],
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Среднее значение функции потерь на обучении 0.013925576582551003\n",
            "Доля верных ответов 0.9991161392964468\n",
            "\n",
            "Среднее значение функции потерь на валидации 0.9857187271118164\n",
            "Доля верных ответов 0.7636749867233139\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jnKSUVrWDZvu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "results1.append({'mode': 'tfidf', 'train_accuracy': accuracy_score(train_source['target'], train_pred.argmax(-1)), 'test_accuracy': accuracy_score(test_source['target'], test_pred.argmax(-1)), 'train_loss': float(train_loss), 'test_loss': float(test_loss)})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6oOPZou3EMxd",
        "colab_type": "text"
      },
      "source": [
        "# RESULTS: top stemming"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZVovopuWEDj5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        },
        "outputId": "eb7ea955-d838-46bc-8619-f5fba4865f3a"
      },
      "source": [
        "pd.DataFrame(results1).sort_values(by=['test_accuracy'], ascending=False)"
      ],
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>mode</th>\n",
              "      <th>train_accuracy</th>\n",
              "      <th>test_accuracy</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>test_loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>pmi</td>\n",
              "      <td>0.999381</td>\n",
              "      <td>0.942777</td>\n",
              "      <td>0.054700</td>\n",
              "      <td>0.303695</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>tf</td>\n",
              "      <td>0.999205</td>\n",
              "      <td>0.811338</td>\n",
              "      <td>0.010306</td>\n",
              "      <td>0.665276</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>tfidf</td>\n",
              "      <td>0.999116</td>\n",
              "      <td>0.763675</td>\n",
              "      <td>0.013926</td>\n",
              "      <td>0.985719</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    mode  train_accuracy  test_accuracy  train_loss  test_loss\n",
              "0    pmi        0.999381       0.942777    0.054700   0.303695\n",
              "1     tf        0.999205       0.811338    0.010306   0.665276\n",
              "2  tfidf        0.999116       0.763675    0.013926   0.985719"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 98
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EqK7l0mgHz-F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nO0tWA8XEovT",
        "colab_type": "text"
      },
      "source": [
        "#N-gram (word 2-gram)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MlCnaLmBEDs-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "results2 = []"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ObctnYo3FqaI",
        "colab_type": "text"
      },
      "source": [
        "#N-gram: VECTORIZATION_MODE: pmi"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lfk-_RugGl5Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_source = fetch_20newsgroups(subset='train')\n",
        "test_source = fetch_20newsgroups(subset='test')\n",
        "\n",
        "train_tokenized = tokenize_corpus(train_source['data'])\n",
        "test_tokenized = tokenize_corpus(test_source['data'])\n",
        "\n",
        "train_tokenized = [[f\"{doc[i_word]}_{doc[i_word+1]}\" for i_word in range(len(doc)-1)] for doc in train_tokenized]\n",
        "test_tokenized = [[f\"{doc[i_word]}_{doc[i_word+1]}\" for i_word in range(len(doc)-1)] for doc in test_tokenized]\n",
        "\n",
        "MAX_DF = 0.8\n",
        "MIN_COUNT = 5\n",
        "vocabulary, word_doc_freq = build_vocabulary(train_tokenized, max_doc_freq=MAX_DF, min_count=MIN_COUNT)\n",
        "\n",
        "train_vectors = vectorize_texts_pmi(train_tokenized, vocabulary, train_source['target'])\n",
        "test_vectors = vectorize_texts_pmi(test_tokenized, vocabulary, test_source['target'])\n",
        "\n",
        "train_dataset = SparseFeaturesDataset(train_vectors, train_source['target'])\n",
        "test_dataset = SparseFeaturesDataset(test_vectors, test_source['target'])\n",
        "\n",
        "UNIQUE_WORDS_N = len(vocabulary)\n",
        "UNIQUE_LABELS_N = len(set(train_source['target']))\n",
        "model = nn.Linear(UNIQUE_WORDS_N, UNIQUE_LABELS_N)\n",
        "\n",
        "scheduler = lambda optim: torch.optim.lr_scheduler.ReduceLROnPlateau(optim, patience=5, factor=0.5, verbose=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BmpYk_q_GmEH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "LR = 1e-1\n",
        "L2 = 1e-3\n",
        "BATCH = 32\n",
        "EPOCH = 200"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vq4QeKtRGmBA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "best_val_loss, best_model = train_eval_loop(model=model,\n",
        "                                            train_dataset=train_dataset,\n",
        "                                            val_dataset=test_dataset,\n",
        "                                            criterion=F.cross_entropy,\n",
        "                                            lr=LR,\n",
        "                                            epoch_n=EPOCH,\n",
        "                                            batch_size=BATCH,\n",
        "                                            l2_reg_alpha=L2,\n",
        "                                            lr_scheduler_ctor=scheduler)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CSiE2mENGtw6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "3478b7b5-dfc9-4d66-b283-d520741e658e"
      },
      "source": [
        "train_pred = predict_with_model(best_model, train_dataset)\n",
        "train_loss = F.cross_entropy(torch.from_numpy(train_pred),torch.from_numpy(train_source['target']).long())\n",
        "test_pred = predict_with_model(best_model, test_dataset)\n",
        "test_loss = F.cross_entropy(torch.from_numpy(test_pred), torch.from_numpy(test_source['target']).long())"
      ],
      "execution_count": 125,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 354/353.5625 [00:02<00:00, 122.93it/s]\n",
            "236it [00:01, 121.88it/s]                             \n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a439WphQGt3-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "9a422296-5369-497b-e053-2b15606842cc"
      },
      "source": [
        "print('Среднее значение функции потерь на обучении', float(train_loss))\n",
        "print('Доля верных ответов', accuracy_score(train_source['target'], train_pred.argmax(-1)))\n",
        "print()\n",
        "print('Среднее значение функции потерь на валидации', float(test_loss))\n",
        "print('Доля верных ответов', accuracy_score(test_source['target'], test_pred.argmax(-1)))"
      ],
      "execution_count": 126,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Среднее значение функции потерь на обучении 0.13543960452079773\n",
            "Доля верных ответов 0.9952271522008131\n",
            "\n",
            "Среднее значение функции потерь на валидации 0.9749530553817749\n",
            "Доля верных ответов 0.7626128518321826\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BfaeVMaaGl9o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "results2.append({'mode': 'pmi', 'train_accuracy': accuracy_score(train_source['target'], train_pred.argmax(-1)), 'test_accuracy': accuracy_score(test_source['target'], test_pred.argmax(-1)), 'train_loss': float(train_loss), 'test_loss': float(test_loss)})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-nZGhrJNGc2h",
        "colab_type": "text"
      },
      "source": [
        "#N-gram: VECTORIZATION_MODE: tf"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lzgPYu-QGmjm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "VECTORIZATION_MODE = 'tf'\n",
        "LR = 1e-1\n",
        "L2 = 0\n",
        "BATCH = 32\n",
        "EPOCH = 200"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yRI5SBlPGmpr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_source = fetch_20newsgroups(subset='train')\n",
        "test_source = fetch_20newsgroups(subset='test')\n",
        "\n",
        "train_tokenized = tokenize_corpus(train_source['data'])\n",
        "test_tokenized = tokenize_corpus(test_source['data'])\n",
        "\n",
        "train_tokenized = [[f\"{doc[i_word]}_{doc[i_word+1]}\" for i_word in range(len(doc)-1)] for doc in train_tokenized]\n",
        "test_tokenized = [[f\"{doc[i_word]}_{doc[i_word+1]}\" for i_word in range(len(doc)-1)] for doc in test_tokenized]\n",
        "\n",
        "MAX_DF = 0.8\n",
        "MIN_COUNT = 5\n",
        "vocabulary, word_doc_freq = build_vocabulary(train_tokenized, max_doc_freq=MAX_DF, min_count=MIN_COUNT)\n",
        "\n",
        "train_vectors = vectorize_texts(train_tokenized, vocabulary, word_doc_freq, mode=VECTORIZATION_MODE)\n",
        "test_vectors = vectorize_texts(test_tokenized, vocabulary, word_doc_freq, mode=VECTORIZATION_MODE)\n",
        "\n",
        "train_dataset = SparseFeaturesDataset(train_vectors, train_source['target'])\n",
        "test_dataset = SparseFeaturesDataset(test_vectors, test_source['target'])\n",
        "\n",
        "UNIQUE_WORDS_N = len(vocabulary)\n",
        "UNIQUE_LABELS_N = len(set(train_source['target']))\n",
        "model = nn.Linear(UNIQUE_WORDS_N, UNIQUE_LABELS_N)\n",
        "\n",
        "scheduler = lambda optim: torch.optim.lr_scheduler.ReduceLROnPlateau(optim, patience=5, factor=0.5, verbose=True)\n",
        "\n",
        "best_val_loss, best_model = train_eval_loop(model=model,\n",
        "                                            train_dataset=train_dataset,\n",
        "                                            val_dataset=test_dataset,\n",
        "                                            criterion=F.cross_entropy,\n",
        "                                            lr=LR,\n",
        "                                            epoch_n=EPOCH,\n",
        "                                            batch_size=BATCH,\n",
        "                                            l2_reg_alpha=L2,\n",
        "                                            lr_scheduler_ctor=scheduler)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "09mlvvexGmuv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "7896ade0-c2ba-4289-c790-53582e7f5b72"
      },
      "source": [
        "train_pred = predict_with_model(best_model, train_dataset)\n",
        "train_loss = F.cross_entropy(torch.from_numpy(train_pred),torch.from_numpy(train_source['target']).long())\n",
        "test_pred = predict_with_model(best_model, test_dataset)\n",
        "test_loss = F.cross_entropy(torch.from_numpy(test_pred), torch.from_numpy(test_source['target']).long())"
      ],
      "execution_count": 130,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 354/353.5625 [00:02<00:00, 121.29it/s]\n",
            "236it [00:01, 123.67it/s]                             \n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OeL44FmjHAOM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "a5ee44f9-52bc-440f-a85f-6f12e792c920"
      },
      "source": [
        "print('Среднее значение функции потерь на обучении', float(train_loss))\n",
        "print('Доля верных ответов', accuracy_score(train_source['target'], train_pred.argmax(-1)))\n",
        "print()\n",
        "print('Среднее значение функции потерь на валидации', float(test_loss))\n",
        "print('Доля верных ответов', accuracy_score(test_source['target'], test_pred.argmax(-1)))"
      ],
      "execution_count": 131,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Среднее значение функции потерь на обучении 0.014156509190797806\n",
            "Доля верных ответов 0.9993812975075128\n",
            "\n",
            "Среднее значение функции потерь на валидации 1.2002676725387573\n",
            "Доля верных ответов 0.6858736059479554\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P6-Alb5EGmzU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "results2.append({'mode': 'tf', 'train_accuracy': accuracy_score(train_source['target'], train_pred.argmax(-1)), 'test_accuracy': accuracy_score(test_source['target'], test_pred.argmax(-1)), 'train_loss': float(train_loss), 'test_loss': float(test_loss)})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y6y9aluGGdAr",
        "colab_type": "text"
      },
      "source": [
        "#N-gram: VECTORIZATION_MODE: tfidf"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Azl63RkGnYY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "VECTORIZATION_MODE = 'tfidf'\n",
        "LR = 1e-1\n",
        "L2 = 0\n",
        "BATCH = 32\n",
        "EPOCH = 200"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ho7aIpGQEDyN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_source = fetch_20newsgroups(subset='train')\n",
        "test_source = fetch_20newsgroups(subset='test')\n",
        "\n",
        "train_tokenized = tokenize_corpus(train_source['data'])\n",
        "test_tokenized = tokenize_corpus(test_source['data'])\n",
        "\n",
        "train_tokenized = [[f\"{doc[i_word]}_{doc[i_word+1]}\" for i_word in range(len(doc)-1)] for doc in train_tokenized]\n",
        "test_tokenized = [[f\"{doc[i_word]}_{doc[i_word+1]}\" for i_word in range(len(doc)-1)] for doc in test_tokenized]\n",
        "\n",
        "MAX_DF = 0.8\n",
        "MIN_COUNT = 5\n",
        "vocabulary, word_doc_freq = build_vocabulary(train_tokenized, max_doc_freq=MAX_DF, min_count=MIN_COUNT)\n",
        "\n",
        "train_vectors = vectorize_texts(train_tokenized, vocabulary, word_doc_freq, mode=VECTORIZATION_MODE)\n",
        "test_vectors = vectorize_texts(test_tokenized, vocabulary, word_doc_freq, mode=VECTORIZATION_MODE)\n",
        "\n",
        "train_dataset = SparseFeaturesDataset(train_vectors, train_source['target'])\n",
        "test_dataset = SparseFeaturesDataset(test_vectors, test_source['target'])\n",
        "\n",
        "UNIQUE_WORDS_N = len(vocabulary)\n",
        "UNIQUE_LABELS_N = len(set(train_source['target']))\n",
        "model = nn.Linear(UNIQUE_WORDS_N, UNIQUE_LABELS_N)\n",
        "\n",
        "scheduler = lambda optim: torch.optim.lr_scheduler.ReduceLROnPlateau(optim, patience=5, factor=0.5, verbose=True)\n",
        "\n",
        "best_val_loss, best_model = train_eval_loop(model=model,\n",
        "                                            train_dataset=train_dataset,\n",
        "                                            val_dataset=test_dataset,\n",
        "                                            criterion=F.cross_entropy,\n",
        "                                            lr=LR,\n",
        "                                            epoch_n=EPOCH,\n",
        "                                            batch_size=BATCH,\n",
        "                                            l2_reg_alpha=L2,\n",
        "                                            lr_scheduler_ctor=scheduler)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DkfYkV3lFnf4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "4715d8f3-6707-4228-a0e5-89c4fb5839d5"
      },
      "source": [
        "train_pred = predict_with_model(best_model, train_dataset)\n",
        "train_loss = F.cross_entropy(torch.from_numpy(train_pred),torch.from_numpy(train_source['target']).long())\n",
        "test_pred = predict_with_model(best_model, test_dataset)\n",
        "test_loss = F.cross_entropy(torch.from_numpy(test_pred), torch.from_numpy(test_source['target']).long())"
      ],
      "execution_count": 135,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 354/353.5625 [00:02<00:00, 123.27it/s]\n",
            "236it [00:01, 122.42it/s]                             \n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NqKYYFLpFnor",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "e0bc88ea-cc2d-4b08-839d-c1173e9ea178"
      },
      "source": [
        "print('Среднее значение функции потерь на обучении', float(train_loss))\n",
        "print('Доля верных ответов', accuracy_score(train_source['target'], train_pred.argmax(-1)))\n",
        "print()\n",
        "print('Среднее значение функции потерь на валидации', float(test_loss))\n",
        "print('Доля верных ответов', accuracy_score(test_source['target'], test_pred.argmax(-1)))"
      ],
      "execution_count": 136,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Среднее значение функции потерь на обучении 0.01566579006612301\n",
            "Доля верных ответов 0.9993812975075128\n",
            "\n",
            "Среднее значение функции потерь на валидации 1.1854768991470337\n",
            "Доля верных ответов 0.7045937334041423\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fsj_AZ4RFnxa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "results2.append({'mode': 'tfidf', 'train_accuracy': accuracy_score(train_source['target'], train_pred.argmax(-1)), 'test_accuracy': accuracy_score(test_source['target'], test_pred.argmax(-1)), 'train_loss': float(train_loss), 'test_loss': float(test_loss)})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aozXcPQTHvtv",
        "colab_type": "text"
      },
      "source": [
        "#RESULTS: top n-gram"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GvDxYOZxFn6l",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        },
        "outputId": "89ccf1b5-c5ad-458e-f678-cbb9a90697e1"
      },
      "source": [
        "pd.DataFrame(results2).sort_values(by=['test_accuracy'], ascending=False)"
      ],
      "execution_count": 138,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>mode</th>\n",
              "      <th>train_accuracy</th>\n",
              "      <th>test_accuracy</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>test_loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>pmi</td>\n",
              "      <td>0.995227</td>\n",
              "      <td>0.762613</td>\n",
              "      <td>0.135440</td>\n",
              "      <td>0.974953</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>tfidf</td>\n",
              "      <td>0.999381</td>\n",
              "      <td>0.704594</td>\n",
              "      <td>0.015666</td>\n",
              "      <td>1.185477</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>tf</td>\n",
              "      <td>0.999381</td>\n",
              "      <td>0.685874</td>\n",
              "      <td>0.014157</td>\n",
              "      <td>1.200268</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    mode  train_accuracy  test_accuracy  train_loss  test_loss\n",
              "0    pmi        0.995227       0.762613    0.135440   0.974953\n",
              "2  tfidf        0.999381       0.704594    0.015666   1.185477\n",
              "1     tf        0.999381       0.685874    0.014157   1.200268"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 138
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pAaCqmbFFoBm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0DWIESHHFoLc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}